

# kubernetes 集群主机准备

## 开启ssh(可选)
部分机器可能没有开启 ssh 服务


1. 更新软件包 
   ```bash
   sudo apt update
   ```
2. 安装ssh服务器
   ```bash
   sudo apt install openssh-server
   ```
3. 设置开机自启
   ```bash
   sudo apt install openssh-server
   ```
4.  确保防火墙允许 SSH 连接
   ```bash
   sudo ufw allow ssh
   ```
   或者，如果你更改了端口，比如改为 2222，则使用：
   ```bash
   sudo ufw allow 2222
   ```



## 主机名配置

设置三台主机名

master
```bash
hostnamectl set-hostname master
```
work01
```bash
hostnamectl set-hostname work01
```


work02
```bash
hostnamectl set-hostname work02
```

查看主机名命令
```bash
hostname
```
## 主机静态IP地址配置(可选)


配置静态IP
```bash
# This file is generated from information provided by the datasource. Changes
# to it will not persist across an instance reboot.To disable cloud-init's
# network configuration capabilities, write a file
#/etc/cloud/cloud.cfg.d/99-disable-network-config.cfg with the following:
# network:fconfig:disabled}network:ethernets:ens33:
network:
   ethernets:
      # 网卡
      ens33:
         # 配置dhcp
         dhcp4: no
         # 配置IP地址
         address:
            - 192.168.10.140/24
         # 网关
         routes:
            - to: default
            via: 192.168.10.2
         # DNS 服务器
         nameservers:
            addresses: [119.29.29.29,29.8.8.8,144.144.144.114]
   version: 2
```

启用配置
```bash
netplan apply
```

使用 `ip a s` 查看


## 主机名与IP地址解析

这是防止后面在签发证书时可能出现的问题（找不到主机等）

```bash
cat >> /etc/hosts << EOF
192.168.1.101 master
192.168.1.102 worker01
192.168.1.103 worker02
EOF
```
查看是否设置成功
```bash
ping -c 2 master
```


## 同步时间

查看时间
```bash
date
```
更换时区
```bash
timedatectl set-timezone Asia/Shanghai
```
安装 ntpdate 命令
```bash
apt install -y ntpdate
```

使用 ntpdate 更新时间
```bash
sudo ntpdate ntp.aliyun.com
```

通过计划任务实现时间同步
```bash
crontab -e
# 然后选择2 进入 vim 编辑器
# 在最后异常输入, 代表在 0 时 0 分 每日 、每月、 每周 执行后面的命令
# m h  dom mon dow   command
0 0 * * * ntpdate ntp.aliyun.com
```

## 配置内核转发及网桥过滤

创建加载内核文件
```bash
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
```

手动加载模块
```bash
sudo modprobe overlay
sudo modprobe br_netfilter
```

查看已经加载的模块
```bash
$ lsmod | grep "overlay"
overlay               147456  0

$ lsmod | grep "br_netfilter"
br_netfilter           28672  0
bridge                299008  1 br_netfilter
```



添加网桥过滤及内核转发配置文件
```bash
# 设置所需的 sysctl 参数，参数在重新启动后保持不变
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF
```

加载内核参数
```bash
# 应用 sysctl 参数而不重新启动
sudo sysctl --system
```
## 安装ipset及ipvsadm

当我们在使用 kube-proxy 代理模式当中，可能会使用到 ipvsadm。

> kube-proxy 的 nftables 后端 ， 在 v1.31 版本中 升级至测试版，通过 NFTTablesProxyMode 功能开关，该功能开关现在默认启用
>
> nftables 是 iptables 的继任者，旨在提供比 uptables 更好的性能和扩展性。 nftables 代理模式比 iptables 模式更快，
> 更有效地处理服务端点更改，并且还能更有效地在内核中处理数据包（尽管这只在数万个服务的集群才能体现出来）
>
> 截止至 kubernetes v1.34 版本， nftables 模式仍然比较新，可能不是与 所有网络插件都兼容。 此代理模式仅在 Linux 上可用，并且需要 内核 5.31或更高版本。
> 某些功能尤其是围绕NodePort服务的功能，在 nftables 模式的视线方式与 iptables 模式中的实现方式并不完全相同


安装 `ipset` 及 `ipvsadm`
```bash
sudo apt install -y ipset ipvsadm
```

配置ipvsadm模块加载

```bash
cat << EOF | tee  /etc/modules-load.d/ipvs.conf
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack
EOF
```
创建加载模块脚本文件
```bash
cat << EOF | tee ipvs.sh
#!/bin/sh
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
EOF
```

给脚本添加执行权限
```bash
chmod +x ipvs.sh
```


执行脚本
```bash
./ipvs.sh
```

使用 lsmod 查看这几个模块是否开启
```bash
lsmod | grep "ip_"
```

## 关闭swap分区
cgroup 在进行资源限制时只能限制真实的内存。如果不关闭 kubeadm 会报错

查看swap 内存

```bash
free -mh

               total        used        free      shared  buff/cache   available
Mem:           4.6Gi       870Mi       2.0Gi        33Mi       1.7Gi       3.4Gi
Swap:             0B          0B          0B
```
临时关闭
```bash
swapoff -a
```
永久关闭
```bash
sudo vi /etc/fstab
```
找到带有 swap 的行，使用 # 注释即可



# kubernetes 集群节点管理工具准备


- Docker
- Containerd
- CRO


## Containerd

下载
```bash
wget https://github.com/containerd/containerd/releases/download/v1.7.20/cri-containerd-1.7.20-linux-amd64.tar.gz
```

解压
```bash
sudo tar xf cri-containerd-1.7.20-linux-amd64.tar.gz -C /
```
查看是否安装
```bash
# which命令是Linux和类Unix操作系统中的一个非常实用的命令，用于查找并显示给定命令的绝对路径。
what01@master:~$ which runc 
/usr/local/sbin/runc
what01@master:~$ which containerd 
/usr/local/bin/containerd

```

### Containerd 配置文件生成并修改

创建配置文件目录
```bash
mkdir /etc/containerd
```
生成配置文件(默认没有配置文件)
```bash
containerd config default > /etc/containerd/config.toml
```

修改第67行：将其中的3.8改为3.9（对此步存疑，在网上未找到版本与k8s版本之间的关系）

```bash
vi  /etc/containerd/config.toml
sandbox_image = "registry.k8s.io/pause:3.8"
```
如果使用阿里云镜像仓库也修改为3.9


继续修改第139行，将其中的 false 改为 true
```bash
SystemdCgroup = false
```
### containerd 启动及开机自启动

设置开机启动并现在启动
```bash
systemctl enable --now containerd

# 如果containerd 已经启动那么/var/run/containerd/ 目录下会出现以下的文件
root@master:/home/what01# ll /var/run/containerd/
总计 0
drwx--x--x  4 root root  120 12月 23 20:50 ./
drwxr-xr-x 39 root root 1080 12月 23 20:50 ../
srw-rw----  1 root root    0 12月 23 20:50 containerd.sock=
srw-rw----  1 root root    0 12月 23 20:50 containerd.sock.ttrpc=
drwx--x--x  2 root root   40 12月 23 20:50 io.containerd.runtime.v1.linux/
drwx--x--x  2 root root   40 12月 23 20:50 io.containerd.runtime.v2.task/

```


验证其版本
```bash
root@master:/home/what01# containerd --version
containerd github.com/containerd/containerd v1.7.20 8fc6bcff51318944179630522a095cc9dbf9f353
```


# kubernetes 集群部署

## 安装源

下载用于 Kubernetes 软件包仓库的公共签名密钥
> 所有仓库都是用相同的签名密钥，因此可以许略URL中的版本


更新 apt 包索引并安装使用 Kubernetes apt 仓库所需要的包：
```bash
sudo apt-get update
# apt-transport-https 可能是一个虚拟包（dummy package）；如果是的话，你可以跳过安装这个包
sudo apt-get install -y apt-transport-https ca-certificates curl gpg
```

下载用于 Kubernetes 软件包仓库的公共签名密钥。所有仓库都使用相同的签名密钥，因此你可以忽略URL中的版本：
```bash
# 如果 `/etc/apt/keyrings` 目录不存在，则应在 curl 命令之前创建它，请阅读下面的注释。
# sudo mkdir -p -m 755 /etc/apt/keyrings
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

# 查看密钥是否存在
ls /etc/apt/keyrings/kubernetes-apt-keyring.gpg
```
> 说明：
> 在低于 Debian 12 和 Ubuntu 22.04 的发行版本中，/etc/apt/keyrings 默认不存在。 应在 curl 命令之前创建它。


添加 Kubernetes apt 仓库。 请注意，此仓库仅包含适用于 Kubernetes 1.31 的软件包； 对于其他 Kubernetes 次要版本，则需要更改 URL 中的 Kubernetes 次要版本以匹配你所需的次要版本 （你还应该检查正在阅读的安装文档是否为你计划安装的 Kubernetes 版本的文档）。
```bash
# 此操作会覆盖 /etc/apt/sources.list.d/kubernetes.list 中现存的所有配置。
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
```

更新apt包索引
```bash
apt-get update
```

## K8s集群软件安装及kubelet配置

所有节点均可安装

### k8s集群软件安装
查看软件列表
```bash
apt-cache policy kubeadm

kubeadm:
  已安装：(无)
  候选： 1.31.4-1.1
  版本列表：
     1.31.4-1.1 500
        500 https://pkgs.k8s.io/core:/stable:/v1.31/deb  Packages
     1.31.3-1.1 500
        500 https://pkgs.k8s.io/core:/stable:/v1.31/deb  Packages
     1.31.2-1.1 500
        500 https://pkgs.k8s.io/core:/stable:/v1.31/deb  Packages
     1.31.1-1.1 500
        500 https://pkgs.k8s.io/core:/stable:/v1.31/deb  Packages
     1.31.0-1.1 500
        500 https://pkgs.k8s.io/core:/stable:/v1.31/deb  Packages
```


查看软件列表及其依赖关系
```bash
apt-cache showpkg kubeadm

Package: kubeadm
Versions: 
1.31.4-1.1 (/var/lib/apt/lists/pkgs.k8s.io_core:_stable:_v1.31_deb_Packages)
 Description Language: 
                 File: /var/lib/apt/lists/pkgs.k8s.io_core:_stable:_v1.31_deb_Packages
                  MD5: dd712e8daa61f5a232c282fd36f21dc9
 Description Language: 
                 File: /var/lib/apt/lists/pkgs.k8s.io_core:_stable:_v1.31_deb_Packages
                  MD5: dd712e8daa61f5a232c282fd36f21dc9
。。。。。。
 
 Description Language: 
                 File: /var/lib/apt/lists/pkgs.k8s.io_core:_stable:_v1.31_deb_Packages
                  MD5: dd712e8daa61f5a232c282fd36f21dc9


Reverse Depends: 
  kubeadm:arm64,kubeadm
  kubeadm:s390x,kubeadm
。。。。。
  kubeadm:s390x,kubeadm
  kubeadm:ppc64el,kubeadm
Dependencies: 
1.31.4-1.1 - cri-tools (2 1.30.0) kubeadm:arm64 (32 (null)) kubeadm:ppc64el (32 (null)) kubeadm:s390x (32 (null)) 
1.31.3-1.1 - cri-tools (2 1.30.0) kubeadm:arm64 (32 (null)) kubeadm:ppc64el (32 (null)) kubeadm:s390x (32 (null)) 
1.31.2-1.1 - cri-tools (2 1.30.0) kubeadm:arm64 (32 (null)) kubeadm:ppc64el (32 (null)) kubeadm:s390x (32 (null)) 
1.31.1-1.1 - cri-tools (2 1.30.0) kubeadm:arm64 (32 (null)) kubeadm:ppc64el (32 (null)) kubeadm:s390x (32 (null)) 
1.31.0-1.1 - cri-tools (2 1.30.0) kubeadm:arm64 (32 (null)) kubeadm:ppc64el (32 (null)) kubeadm:s390x (32 (null)) 
Provides: 
1.31.4-1.1 - 
1.31.3-1.1 - 
1.31.2-1.1 - 
1.31.1-1.1 - 
1.31.0-1.1 - 
Reverse Provides: 
```

查看多个软件可用列表
```bash
root@master:/home/what01# apt-cache madison kubeadm kubelet kubectl
   kubeadm | 1.31.4-1.1 | https://pkgs.k8s.io/core:/stable:/v1.31/deb  Packages
。。。。。
   kubectl | 1.31.0-1.1 | https://pkgs.k8s.io/core:/stable:/v1.31/deb  Packages
```

默认安装(-y参数是自动回答提示为yes)
```bash
apt-get install -y kubelet kubeadm kubectl
```

指定版本安装
```bash
apt-get install -y kubelet=1.31.0-1.1 kubeadm=1.31.0-1.1 kubectl=1.31.0-1.1
```


锁定版本防止自动更新
```bash
apt-mark hold kubelet kubeadm kubectl
```


解锁定版本可以自动更新
```bash
apt-mark unhold kubelet kubeadm kubectl
```

### kubelet 配置


确保 kubelet 使用 systemd 作为 cgroup 驱动

kubeadm 是 Kubernetes 的官方部署工具，它依赖于 systemd 来管理服务。也是为了实现容器运行时使用的 cgroupdriver 与 kubelet 使用的 cgroup 的一致性。
```bash
# 从1.30版本kubelet的配置文件在以下路径 /etc/default/kubelet
# vi /etc/default/kubelet
KUBELET_EXTRA_ARGS="--cgroup-driver=systemd"
```


设置kubelet开机自启动，由于没有生成配置文件，集群初始化后自动启动 
```bash
systemctl enable kubelet
```


## k8s集群初始化


### 查看版本
```bash
kubeadm version
```

### 生成部署配置文件
准备集群初始化的配置文件



先生成基础的配置文件
```bash
kubeadm config print init-defaults > kubeadm-config.yaml
```
生成的配置文件
```yaml
apiVersion: kubeadm.k8s.io/v1beta4
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  # 本地API端点地址
  # 将其修改为本机地址
  advertiseAddress: 192.168.1.101
  bindPort: 6443
nodeRegistration: # 节点的注册情况
  # 当前使用的容器是 containerd 所以无需修改
  criSocket: unix:///var/run/containerd/containerd.sock
  imagePullPolicy: IfNotPresent
  imagePullSerial: true
  # 当前节点名称
  name: k8s-master01
  taints: null
timeouts:
  controlPlaneComponentHealthCheck: 4m0s
  discovery: 5m0s
  etcdAPICall: 2m0s
  kubeletHealthCheck: 4m0s
  kubernetesAPICall: 1m0s
  tlsBootstrap: 5m0s
  upgradeManifests: 5m0s
---
apiServer: {}
apiVersion: kubeadm.k8s.io/v1beta4
caCertificateValidityPeriod: 87600h0m0s
certificateValidityPeriod: 8760h0m0s
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns: {}
encryptionAlgorithm: RSA-2048
etcd:
  # 如果部署到集群外，这里就需要改变
  local:
    dataDir: /var/lib/etcd
# 容器镜像仓库
imageRepository: registry.k8s.io
kind: ClusterConfiguration
# 当前集群的版本
kubernetesVersion: 1.31.0
networking:
  # 默认的域名 
  dnsDomain: cluster.local
  # serviceSubnet是Kubernetes服务（Service）所使用的子网。在Kubernetes中，Service是一种抽象层，它定义了一个逻辑集合和访问它们的策略。Service允许你访问一组运行在一个或多个Pods上的应用程序
  serviceSubnet: 10.96.0.0/12
  # podSubnet是Pod所使用的子网。Pod是Kubernetes中可以创建和管理的最小部署单元，它包含一个或多个容器。   
  podSubnet: 10.244.0.0/16
proxy: {}
scheduler: {}
```
### 查看并下载镜像

列出镜像

```bash
root@master:/home/what01# kubeadm config images list
I1226 14:17:26.802088 3214421 version.go:261] remote version is much newer: v1.32.0; falling back to: stable-1.31
registry.k8s.io/kube-apiserver:v1.31.4
registry.k8s.io/kube-controller-manager:v1.31.4
registry.k8s.io/kube-scheduler:v1.31.4
registry.k8s.io/kube-proxy:v1.31.4
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.10
registry.k8s.io/etcd:3.5.15-0
```
列出符合该版本的镜像
```bash
root@master:/home/what01# kubeadm config images list --kubernetes-version=v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.10
registry.k8s.io/etcd:3.5.15-0
```




拉取镜像
```bash
kubeadm config images pull
```

指定 k8s 版本然后拉取符合该版本的镜像
```bash
kubeadm config images pull --kubernetes-version=v1.31.0
```

当镜像拉取成功后可以使用 `crictl images` 查看已拉取的镜像
```bash
root@master:/home/what01# crictl images
IMAGE                                     TAG                 IMAGE ID            SIZE
registry.k8s.io/etcd                      3.5.15-0            2e96e5913fc06       56.9MB
registry.k8s.io/kube-apiserver            v1.31.0             604f5db92eaa8       28.1MB
registry.k8s.io/kube-controller-manager   v1.31.0             045733566833c       26.2MB
registry.k8s.io/kube-proxy                v1.31.0             ad83b2ca7b09e       30.2MB
registry.k8s.io/kube-scheduler            v1.31.0             1766f54c897f0       20.2MB
registry.k8s.io/pause                     3.8                 4873874c08efc       311kB
```
### 使用部署配置文件初始化 k8s 集群

```bash
kubeadm init --config kubeadm-config.yaml --upload-certs --v=9
```
当执行该命令时会看到一系列的信息输出
```bash
# 当看到该输出时说明 控制平面已经初始化成功
Your Kubernetes control-plane has initialized successfully!

# 然后执行以下的命令，完成配置
To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

# 通过工作节点的 root 用户，加入集群
Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.1.101:6443 --token abcdef.0123456789abcdef \
        --discovery-token-ca-cert-hash sha256:996eec9e08da352228ec36da9282d95b2906636ec4b9c074976d7f889981fe9c
```

通过 `kubectl get nodes` 命令可以查看当前的节点
```bash
root@master:/home/what01# kubectl get nodes
NAME           STATUS     ROLES           AGE   VERSION
k8s-master01   NotReady   control-plane   81s   v1.31.0
```

在工作节点上执行加入命令后再次查看
```bash
root@master:/home/what01# kubectl get nodes
NAME           STATUS     ROLES           AGE   VERSION
k8s-master01   NotReady   control-plane   21h   v1.31.0
worker01       NotReady   <none>          21h   v1.31.0
worker02       NotReady   <none>          21h   v1.31.0
```
但是目前所有节点都是 `NotReady` 状态，代表目前无法对集群做出任何调度

通过 `kubectl get cs` 命令查看集群中组件的状态信息
```bash
root@master:/home/what01# kubectl get cs 
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE   ERROR
scheduler            Healthy   ok        
controller-manager   Healthy   ok        
etcd-0               Healthy   ok   
```
通过 `kubectl get pods -n kube-system` 查看 `kube-system` 命名空间下所有 `pod`
```bash
root@master:/home/what01# kubectl get pods -n kube-system 
NAME                             READY   STATUS    RESTARTS      AGE
coredns-6f6b679f8f-7459m         0/1     Pending   0             13m
coredns-6f6b679f8f-nnjhd         0/1     Pending   0             13m
etcd-master                      1/1     Running   0             13m
kube-apiserver-master            1/1     Running   2 (14m ago)   13m
kube-controller-manager-master   1/1     Running   0             13m
kube-proxy-ts6h2                 1/1     Running   0             13m
kube-scheduler-master            1/1     Running   0             13m
```
通过 `top` 命令可以看到 `kubelet` 、`kube-controller` 、`kube-apiserver` 、`kube-scheduler` 、`kube-proxy` 、`etcd` 等进程
```bash
root@master:/home/what01# top | grep kube 
  28814 root      20   0 1312120 107992  65168 S   6.2   1.3   0:13.50 kube-controller                                                      
  29807 root      20   0 1452952 249596  72492 S   3.7   3.0   0:26.91 kube-apiserver                                                       
  29946 root      20   0 2413576  92516  59776 S   2.3   1.1   0:13.69 kubelet                                                              
  30190 root      20   0 1292176  52300  41660 S   0.3   0.6   0:00.23 kube-proxy 
  28821 root      20   0 1291500  60508  44932 S   0.3   0.7   0:03.12 kube-scheduler                                                       

root@master:/home/what01# top | grep etcd 
  29524 root      20   0   11.2g  51628  22132 S   2.3   0.6   0:41.91 etcd 
```




# kubernetes 集群网络插件部署

- Flannel
- Calico
- Cilium
  
 Flannel 适合小规模的 k8s 集群，如果集群超过 20 台建议使用 Calico ，如果集群属于多集群 那么推荐使用 Cilium

## 基于 Calico 部署

Calico 官方文档

<https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart#install-calico>


执行该命令
```bash
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.29.1/manifests/tigera-operator.yaml
```
查看命名空间 , 会出现一个 `tigera-operator` 的命名空间
```bash
root@master:/home/what01# kubectl get ns 
NAME              STATUS   AGE
default           Active   56m
kube-node-lease   Active   56m
kube-public       Active   56m
kube-system       Active   56m
tigera-operator   Active   8m2s
```
查看该命名空间下的 `pod` 
```bash
root@master:/home/what01# kubectl get pods -n tigera-operator
NAME                               READY   STATUS    RESTARTS   AGE
tigera-operator-76c4976dd7-ht9q5   1/1     Running   0          9m50s
```
当所有 `pod` 为 `Running` 状态时，执行后面的步骤




下载配置文件修改网段
```bash
wget https://raw.githubusercontent.com/projectcalico/calico/v3.29.1/manifests/custom-resources.yaml
```
配置文件 修改
```bash
# This section includes base Calico installation configuration.
# For more information, see: https://docs.tigera.io/calico/latest/reference/installation/api#operator.tigera.io/v1.Installation
apiVersion: operator.tigera.io/v1
kind: Installation
metadata:
  name: default
spec:
  # Configures Calico networking.
  calicoNetwork:
    ipPools:
    - name: default-ipv4-ippool
      blockSize: 26
      # 将其修改为上面的 podSubnet 同样的网段
      cidr: 10.244.0.0/16
      encapsulation: VXLANCrossSubnet
      natOutgoing: Enabled
      nodeSelector: all()

---

# This section configures the Calico API server.
# For more information, see: https://docs.tigera.io/calico/latest/reference/installation/api#operator.tigera.io/v1.APIServer
apiVersion: operator.tigera.io/v1
kind: APIServer
metadata:
  name: default
spec: {}
```
执行配置文件
```bash
root@master:/home/what01# kubectl create -f custom-resources.yaml
installation.operator.tigera.io/default created
apiserver.operator.tigera.io/default created
```
同样会多出命名空间
```bash
root@master:/home/what01# kubectl get ns 
NAME               STATUS   AGE
calico-apiserver   Active   10s
calico-system      Active   10s
default            Active   60m
kube-node-lease    Active   60m
kube-public        Active   60m
kube-system        Active   60m
tigera-operator    Active   12m
```

查看这两个命名空间
```bash
root@master:/home/what01# kubectl get pods -n calico-apiserver
NAME                                READY   STATUS              RESTARTS   AGE
calico-apiserver-786b6b85dd-7p8qc   0/1     ContainerCreating   0          87s
calico-apiserver-786b6b85dd-sh2jr   0/1     ContainerCreating   0          87s

root@master:/home/what01# kubectl get pods -n calico-system
NAME                                       READY   STATUS              RESTARTS   AGE
calico-kube-controllers-5446f8d879-vlg99   1/1     Running             0          2m1s
calico-node-287cp                          0/1     Running             0          2m1s
calico-typha-b9b4dc9-z2gmn                 1/1     Running             0          2m2s
csi-node-driver-fnkc5                      0/2     ContainerCreating   0          2m1s
```






# 部署应用验证 kubernetes 集群可用性


验证 coredns
```bash
dig -t a www.baidu.com @10.96.0.10
```
输出
```bash

# 如果包含以下类似输出说明能够正常解析
;; ANSWER SECTION:
www.baidu.com.          5       IN      CNAME   www.a.shifen.com.
www.a.shifen.com.       5       IN      A       39.156.70.46
www.a.shifen.com.       5       IN      A       39.156.70.239
```