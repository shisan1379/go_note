
# 服务暴露

## ClusterIp集群内部服务

kubenetes 默认的服务类型，为 Pod 提供集群内的虚拟 IP 地址，仅在集群内部可访问。

**特点：**

- 自动分配集群内部IP，如 10.96.0.0/12 网段
- 无法从集群外部访问
- 用于集群内部微服务间的通信
**示例**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-internal-service
spec:
  type: ClusterIP  # 默认类型，可省略
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80          # 服务在集群内部暴露的端口（集群内其他组件通过此端口访问服务）。
      targetPort: 8080  # 后端 Pod 上应用实际监听的端口（流量将被转发至此）。
```


## NodePort集群外部访问
在每个Node节点上开放固定端口（默认范围30000 ~ 32767），通过 NodeIp:NodePort 从集群外部访问服务

**特点**
- 基于 ClusterIP 实现，自动创建 ClusterIP
- 端口必须唯一，不支持动态分配
- 适合测试环境和小规模服务暴露

示例：

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-nodeport-service
spec:
  type: NodePort
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80        # 服务端口（ClusterIP端口）
      targetPort: 8080  # Pod端口
      nodePort: 30080   # 节点端口（可选，不指定则自动分配）
```

## ExternalIP外部IP访问

通过节点的物理IP地址暴露服务，需要手动配置外部IP（如公有云或物理公网IP）

**特点**
- 需要确保 ExternalIP 可路由到集群节点
- 请求直接转发到服务，无需NAT
- 适合混合云或物理集群

示例
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-external-service
spec:
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
  externalIPs:
    - 203.0.113.10  # 节点的外部IP地址
```


## HostPort容器直接映射到节点
将Pod端口直接映射到Node节点所在的端口，通过 NodeIP：NodePort访问

**特点**：
- 不使用 Service 直接通过节点IP暴露Pod
- 同一端口在同一节点上只能被一个Pod使用
- 适合需要直接访问节点网络的场景，如监控、日志收集等


**示例**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-container
    image: nginx
    ports:
    - containerPort: 80
      hostPort: 8080  # 直接映射到节点的8080端口
```


## 对比



| 类型           | 访问范围         | 实现方式                 | 端口范围     | 适用场景                   |
| -------------- | ---------------- | ------------------------ | ------------ | -------------------------- |
| **ClusterIP**  | 集群内部         | 虚拟IP，负载均衡         | 任意（如80） | 内部微服务通信             |
| **NodePort**   | 集群外部         | 节点固定端口 + ClusterIP | 30000-32767  | 测试环境、小规模暴露服务   |
| **ExternalIP** | 集群外部         | 节点物理IP + Service     | 任意         | 混合云、物理机集群         |
| **HostPort**   | 节点级别外部访问 | Pod直接映射到节点        | 任意         | 特殊网络需求（如节点监控） |


## 总结

- **优先使用ClusterIP**：微服务间通信的首选方式。
- **NodePort适合快速暴露服务**：无需额外负载均衡器，适合开发和测试。
- **ExternalIP依赖网络环境**：需确保外部IP可路由。
- **HostPort谨慎使用**：可能导致端口冲突，仅用于特殊场景。

理解这些概念有助于设计合理的Kubernetes网络架构，实现服务间的高效通信和外部访问。




# Service的作用

使用 kubernetes 集群运行工作负载时，由于 Pod 经常处于用后即焚的状态， Pod 经常被重新生成，因此Pod 对应的地址也会经常变化，导致无法直接访问 Pod 提供的服务。 kubernetes 使用 service 来解决这一问题。 即在 Pod 前面使用 Service 对 Pod 进行代理，无论 Pod 怎样变化，只要有 Label ，就可以让 Service 联系上 Pod。 把 Pod Ip 地址 添加到 Service 对应的端点列表（Endpoints） 实现对PodIP跟踪，进而实现通过 Service 访问 Pod 目的。



- 通过 Service 为 Pod 客户端提供访问 Pod 的方法
- 通过标签动态感知 Pod IP 地址变化等
- 防止 Pod 失联
- 定义 Pod 访问策略
- 通过 label-selector 相关联
- 通过 Service 实现 Pod 的负载均衡 （TCP/UDP 4层）
- 底层实现由 kube-proxy 通过 userspace 、 iptables 、 ipvs 三种代理模式









# kube-proxy的三种代理模式


- kubernetes 集群中有三层网络
  - 一类是真实的，例如 Node Network ， Pod Network 提供真实IP地址
  - 一类是虚拟的， 例如 Cluster Network 、 Service Network，不会出现在接口上，仅会出现在 Service 中

- kube-proxy 始终监控（watch） kube-apiserver 上关于Service相关的资源变动状态，一旦获取相关信息， kube-proxy 都要把相关信息转换为当前节点上能够实现Service资源调度的规则，进而实现访问Service就能够获取Pod所提供的服务

- kube-proxy 三种代理模式， UserSpace 模式 、 iptables 模式、 ipvs模式



## UserSpace 模式

UserSpace 是 kube-proxy 最早的实现模式，基于用户空间进程完成流量转发，目前已被弃用（K8s v1.20 后完全移除）。

### 工作原理


**流量路径**

1. kube-proxy 会为每个 Service 随机监听一个端口 proxy-port ，并增加一条 iptables 规则。
2. 客户端请求，先通过 iptables 规则重定向到 用户空间的 kube-proxy 进程（就是发送到 proxy-port），再由 kube-proxy 根据负载均衡算法（如轮询）转发到后端 pod
3. 响应流量原路返回（先到 kube-proxy，再经 iptables 回传给客户端）
```
客户端 → 内核 iptables（重定向） → 用户空间 kube-proxy 进程 → 后端 Pod
后端 Pod 响应 → kube-proxy 进程 → 内核 iptables → 客户端
```



**核心逻辑**

kube-proxy 进程持续监听 API Server ，同步 Service 和 Endpoint 变化，并动态调整 iptables 规则 和自身的转发策略



### 特点

**优点**
1. 兼容性强，不依赖特定内核功能，支持老旧 Linux 发行版
2. 用户空间实现负载均衡算法
3. 调试简单，流量在用户空间可见，可以通过 netstat 等工具直接监控


**缺点**
1. 性能极差，每次转发需经过用户空间和内核空间的两次切换，延迟高，吞吐量低
2. 单点风险， kube-proxy 进程崩溃会导致 节点上的 service 失效
3. 资源消耗高， 用户空间进程处理大量连接时占用 CPU/内存较高

## iptables 模式

iptables 模式是 Kubernetes v1.8 后的默认模式（替代 UserSpace），基于 Linux 内核的 iptables 规则实现负载均衡，直接在内核空间处理流量。


### 工作原理

1. kube-proxy 监听 API Server 中 Service 和 Endpoint 的变化，动态生成 iptables 规则（存储在内核中）
2. 客户端请求直接通过内核的iptables规则匹配并转发到后端pod，无需经过用户空间

```
客户端 → 内核 iptables 规则（直接转发） → 后端 Pod
后端 Pod 响应 → 内核 iptables 规则 → 客户端
```


### 特点


**优点**
1. 完全是基于内核空间的iptables转发，避免用户空间切换开销
2. 部署简单，大多数 Linux 都支持 iptables ，无需额外配置

**缺点**
1. 规则匹配效率低， uptables 规则是 线性匹配（类似链表），当 Service 数量庞大时（数千个），规则匹配延迟会显著增加
2. 功能有限： 不支持高级的负载均衡算法（如加权轮询，最少连接等），也不支持会话保持（需要额外配置）
3. 故障恢复慢： 当后端 pod 故障时，需要依赖 kube-proxy 重新生成 iptables 规则，恢复延迟较高




## IPVS 模式
IPVS（IP Virtual Server）是 Linux 内核的负载均衡模块，专为大规模服务设计。Kubernetes v1.8 引入 IPVS 模式作为 iptables 的高性能替代方案，需手动启用。



### IPVS模式的核心机制

IPVS模式下 kube-proxy 会
1. 通过 ipvsadm 配置 IPVS 规则
   - 根据 Service 创建 虚拟服务(Virtual Service), 对应 ClusterIp 和 端口
   - 根号有 Endpoints 创建 真实服务器（Real Server），对应后端 Pod 的 Id 和端口
   - 配置负载均衡算法（如轮询、最少连接数）
2. IPVS 直接在内核层转发流量
   - 客户请求到达节点后，IPVS 直接将流量转发至后端 Pod（基于IPVS规则），无需经过用户空间



### 工作原理







**流量路径**

1. kube-proxy 启动时会检查并加载 ip_vs 模块，然后根据 Service 和 Endpoint 创建 IPVS 虚拟服务器（Virtual Server 对应 Service 的 Cluster IP） 和后端真实服务器（Real Server 对应的 Pod IP）; 客户端请求直接由 IPVS 在内核空间转发到后端 Pod

```
客户端 → 内核 IPVS 规则（直接转发） → 后端 Pod
后端 Pod 响应 → 内核 IPVS 规则 → 客户端
```



### 特点


**优点**
1. 性能最优，内核级转发，哈希表查询，支持百万并发连接，接近于硬件负载均衡，
2. 支持大规模集群： 规则查询效率高，适合数千个甚至数万个 Service 的场景
3. 功能丰富： 支持多种负载均衡算法和会话保持
4. 故障恢复快： 内置健康检查，后端 Pod 故障可快速移除


**缺点**
1. 依赖内核模块： 需要Linux 启动 ip_vs 模块，通常需要手动加载
2. 配置复杂： 需要提前安装 ipset 和 ipvsadm工具， 且需要内核支持


### IPVS模式下 iptables 的使用场景

尽管 IPVS 负责主要的负载均衡，但以下场景仍然依赖iptables

1. 网络策略层（Network Policy）
   - 功能： 控制 Pod 间的流量访问规则（如允许、拒绝特定 Pod 通信）
   - 实现： 完全依赖 iptables 规则，与 IPVS 无关
   - 示例: 此策略会生成 iptables规则，限制只有带 role=frontend 标签的 Pod 能访问 role=db 的 Pod。
    ```yaml
    apiVersion: networking.k8s.io/v1
    kind: NetworkPolicy
    spec:
      podSelector:
        matchLabels:
          role: db
      ingress:
        - from:
            - podSelector:
                matchLabels:
                  role: frontend
    ```

2. SNAT（源地址转换）
   - 功能： 当 Pod 访问集群外部服务时，将源 IP 从 Pod IP 转换为 Node IP，确保响应能正确返回。

   - 实现： 通过 iptables 的 POSTROUTING 链实现，示例规则：
     ```bash
      -A POSTROUTING ! -d 10.0.0.0/16 -m comment --comment "kubernetes postrouting rules" -j MASQUERADE
     ```
     此规则会将所有发往集群外部（非 10.0.0.0/16）的流量进行 SNAT。


3. 特殊流量的转发规则
   - 功能： 处理 IPVS 未覆盖的边缘场景（如 HostPort 、NodePort、ExternalIP 等）。
   - 实现： 通过 iptables 规则将流量导向 IPVS 或直接转发，示例规则：
     ```bash
     # NodePort 流量先通过 iptables 转发到 ClusterIP
    -A KUBE-NODEPORTS -p tcp -m comment --comment "Kubernetes service nodeports; NOTE: this must be the last rule in this chain" -m tcp --dport 30080 -j KUBE-SVC-ABCDEFG
     ```

4. 健康检查与探针
   - 功能：kubelet 的健康检查（如 livenessProbe、readinessProbe）可能依赖 iptables 规则进行流量拦截和转发。
   - 实现：通过 iptables 规则将探针请求导向目标 Pod。


### IPVS 与 iptables 的协作流程

1. 集群内部流量
   ```
   客户端 -> IPVS规则（直接转发至Pod， 无需iptables）
   ```

2. 集群外部流量（NodePort -> Pod）
   ```
   客户端 -> iptables规则（NodePort -> ClusterIp） -> IPVS规则 -> Pod
   ```
   
3. pod 访问外部服务
   ```
   Pod -> iptables规则（SNAT转换源IP） -> 网络 -> 外部服务
   ```

### 验证IPVS 与 iptables 
```bash
# 查看 IPVS 规则（负载均衡）
sudo ipvsadm -L -n

# 查看 iptables 规则（网络策略、SNAT 等）
sudo iptables-save | grep -E "KUBE|calico"  # 或其他 CNI 插件相关规则
```
### IPVS维护的ipset集合

| 设置名称                           | 成员                                                                    | 用法                                                                                                                                |
| ---------------------------------- | ----------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- |
| **KUBE-CLUSTER-IP**                | 所有ClusterIP服务的IP+端口                                              | 当`masquerade-all=true`或指定`clusterCIDR`时，对发往Service ClusterIP的流量进行SNAT（源地址转换），确保返回流量能正确路由回客户端。 |
| **KUBE-LOOP-BACK**                 | 所有服务IP+端口+IP（源IP）                                              | 解决**数据包循环**问题。当Pod访问自己提供的Service时，避免流量绕出集群再返回，直接在集群内部环路转发。                              |
| **KUBE-EXTERNAL-IP**               | 服务ExternalIP+端口                                                     | 对发往Service ExternalIP的流量进行SNAT，将源IP伪装为节点IP，确保响应流量能正确返回。                                                |
| **KUBE-LOAD-BALANCER**             | LoadBalancer类型服务的入口IP+端口                                       | 对发往LoadBalancer服务的流量进行SNAT，将源IP伪装为节点IP，适用于`externalTrafficPolicy=Cluster`的场景。                             |
| **KUBE-LOAD-BALANCER-LOCAL**       | LoadBalancer类型服务的入口IP+端口（仅`externalTrafficPolicy=local`）    | 对发往`externalTrafficPolicy=local`的LoadBalancer服务的流量进行DNAT（目的地址转换），直接转发到本地节点的Endpoint，保留客户端源IP。 |
| **KUBE-LOAD-BALANCER-FW**          | LoadBalancer类型服务的入口IP+端口（仅配置了`loadBalancerSourceRanges`） | 根据`loadBalancerSourceRanges`定义的源IP CIDR范围，丢弃不符合条件的流量，实现访问控制。                                             |
| **KUBE-LOAD-BALANCER-SOURCE-CIDR** | LoadBalancer类型服务的入口IP+端口+源CIDR                                | 允许来自`loadBalancerSourceRanges`指定CIDR范围的流量访问LoadBalancer服务，与`KUBE-LOAD-BALANCER-FW`配合实现黑白名单。               |
| **KUBE-NODE-PORT-TCP**             | NodePort类型服务的TCP端口                                               | 对发往NodePort（TCP）的流量进行SNAT，将源IP伪装为节点IP，适用于`externalTrafficPolicy=Cluster`的场景。                              |
| **KUBE-NODE-PORT-LOCAL-TCP**       | NodePort类型服务的TCP端口（仅`externalTrafficPolicy=local`）            | 对发往`externalTrafficPolicy=local`的NodePort（TCP）流量进行DNAT，直接转发到本地节点的Endpoint，保留客户端源IP。                    |
| **KUBE-NODE-PORT-UDP**             | NodePort类型服务的UDP端口                                               | 功能同`KUBE-NODE-PORT-TCP`，但针对UDP协议。                                                                                         |
| **KUBE-NODE-PORT-LOCAL-UDP**       | NodePort类型服务的UDP端口（仅`externalTrafficPolicy=local`）            | 功能同`KUBE-NODE-PORT-LOCAL-TCP`，但针对UDP协议。                                                                                   |







### IPVS 支持以下常用调度算法

在 Kubernetes 中，当 Service 使用 `type: ClusterIP` 或 `type: NodePort` 时，若 kube-proxy 配置为 IPVS 模式，可通过修改 Service 的注解（annotation）来调整其负载均衡调度方式。IPVS 提供了多种调度算法，适用于不同场景。


| 调度算法 | 说明                                                                             | 适用场景                                 |
| -------- | -------------------------------------------------------------------------------- | ---------------------------------------- |
| `rr`     | 轮询（Round-Robin）：依次分配请求到后端 Pod                                      | 无状态服务，希望请求均匀分布             |
| `wrr`    | 加权轮询（Weighted Round-Robin）：按权重分配，权重高的 Pod 接收更多请求          | 需要按能力分配负载（如不同规格的 Pod）   |
| `lc`     | 最少连接（Least-Connection）：优先分配到连接数最少的 Pod                         | 长连接服务（如 TCP 长连接）              |
| `wlc`    | 加权最少连接（Weighted Least-Connection）：结合权重和连接数                      | 需按权重和负载综合分配                   |
| `sh`     | 源哈希（Source Hashing）：基于客户端 IP 哈希固定到某 Pod（类似 SessionAffinity） | 需要会话亲和性的场景                     |
| `dh`     | 目的哈希（Destination Hashing）：基于目标 IP 哈希分配                            | 缓存服务（如代理后端相同内容到固定 Pod） |
| `sed`    | 最短预期延迟（Shortest Expected Delay）：优先分配到响应最快的 Pod                | 对响应时间敏感的服务                     |
| `nq`     | 永不排队（Never Queue）：避免请求排队，直接分配到可用 Pod                        | 高并发场景，减少请求等待                 |


### 指定调度算法

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
  annotations:
    service.kubernetes.io/ipvs-scheduler: "wrr"  # 指定调度算法为加权轮询
spec:
  selector:
    app: my-app
  ports:
  - port: 80
    targetPort: 8080
  type: ClusterIP
```



## 三种模式对比


| **对比维度**         | **UserSpace 模式**          | **iptables 模式**              | **IPVS 模式**                  |
| -------------------- | --------------------------- | ------------------------------ | ------------------------------ |
| **工作空间**         | 用户空间（kube-proxy 进程） | 内核空间（iptables 规则）      | 内核空间（IPVS 模块）          |
| **转发性能**         | 最低（两次上下文切换）      | 中（线性规则匹配）             | 最高（哈希表查询）             |
| **负载均衡算法**     | 基础（轮询）                | 有限（随机、轮询）             | 丰富（RR、WRR、LC、SH 等）     |
| **规则查询效率**     | 低（用户空间处理）          | 中（线性遍历）                 | 高（O(1) 哈希查询）            |
| **大规模支持**       | 差（性能瓶颈明显）          | 一般（数千 Service 可能卡顿）  | 优（数万 Service 无压力）      |
| **故障恢复速度**     | 慢（依赖进程重启）          | 中（依赖 kube-proxy 更新规则） | 快（内置健康检查）             |
| **依赖组件**         | iptables（仅重定向）        | iptables、ipset                | ip_vs 内核模块、ipset、ipvsadm |
| **适用场景**         | 遗留环境、调试              | 中小规模集群（默认选择）       | 大规模集群、高性能需求         |
| **K8s 支持状态**     | 已弃用（v1.20+ 移除）       | 稳定（默认模式）               | 稳定（推荐大规模场景）         |
| **典型延迟（请求）** | 数百微秒（甚至毫秒级）      | 数十微秒                       | 数微秒                         |



## 查看当前使用的模式

```bash
# 获取 kube-proxy Pod 名称
kubectl get pods -n kube-system | grep kube-proxy

# 查看日志（替换 POD_NAME）
kubectl logs -n kube-system POD_NAME | grep "Using"
```




# Service类型 

- ClusterIP
  - 默认，分配一个集群内部可以访问的虚拟IP
- NodePort
  - 在每个Node上分配一个端口作为外部访问入口
  - nodePort端口范围 30000 ~ 32767
- LoadBalancer
  - 工作在特定的 Cloud Provider 上。例如 Google Cloud 、AWS 、Open Stack
- ExternalName
  - 表示把集群外部的服务引入到集群内部来，即实现了集群内部Pod和集群外部的服务进行通信

## Service 参数

- port ：访问Service使用的端口
- targetPort ： Pod中容器端口
- nodePort  ： 通过Node实现外网用户访问k8s集群内Service（30000 ~ 32767）


# Service创建

## ClusterIp类型

ClusterIP根据是否生成ClusterIp又可分为两类


- 普通Service，被分配一个集群内部可访问的固定虚拟IP（Cluster IP）， 实现集群内部的访问
- HeadlessService，不会分配 ClusterIP也不通过 kube-proxy 做反向代理和负载均衡，而是通过DNS和稳定的网络ID来访问。DNS 会将headless Service 的后端直接解析为 Pod Ip 列表


### 生成Service




使用命令行生成Service
```bash
kubectl expose deployment <deployment-name> --type=ClusterIP --target-port=80 --port=80
```
使用yaml文件生成Service(示例)
```bash
kubectl apply -f xxx.service
```

Service 的 yaml 配置文件
```yaml
apiVersion: v1        # 指定 Kubernetes API 版本，这里是核心 API 组的 v1 版本
kind: Service         # 指定这是一个 Service 对象
metadata:
  name: nginx-deployment-service         # Service 的名称
spec:
  type: ClusterIP      # Service 的类型，NodePort 类型允许从集群外部访问 Service
  ports:
    - port: 80      # Service 暴露在集群内部的端口
      targetPort: 80 # 流量将被转发到的后端 Pod 的端口
  selector:           # 用于选择哪些 Pod 将被这个 Service 暴露
    app: nginx-deployment        # 标签选择器，匹配具有 app=myapp 标签的 Pod
```


查看关联关系
```bash
root@master:~/.kube# kubectl get pods  -o wide 
NAME                               READY   STATUS    RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES
nginx-deployment-9957b6647-5v2nl   1/1     Running   0          32m   10.244.140.69    node02   <none>           <none>
nginx-deployment-9957b6647-j5c45   1/1     Running   0          32m   10.244.196.130   node01   <none>           <none>
```

然后继续查看 Service
```bash
root@master:~/.kube# kubectl get service  -o wide 
NAME                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE     SELECTOR
kubernetes                 ClusterIP   10.96.0.1       <none>        443/TCP   2d17h   <none>
nginx-deployment-service   ClusterIP   10.98.182.100   <none>        80/TCP    33m     app=nginx-deployment




root@master:~/.kube# kubectl describe service nginx-deployment-service
Name:                     nginx-deployment-service
Namespace:                default
# 服务自身的标签，用于分类和选择资源
Labels:                   <none>
# 附加元数据，通常用于控制器或工具读取配置
Annotations:              <none>
# 标签选择器，用于匹配后端Pod
Selector:                 app=nginx-deployment
# 服务类型，ClusterIP表示仅在集群内部可访问
Type:                     ClusterIP
# IP地址族策略，SingleStack表示仅使用单一IP地址族
IP Family Policy:         SingleStack
IP Families:              IPv4
# 集群内部分配给Service的虚拟IP地址
IP:                       10.98.182.100
# 服务的IP地址列表（单IP时与IP字段相同）
IPs:                      10.98.182.100
# 服务对外暴露的端口
Port:                     <unset>  80/TCP
# 后端Pod接收流量的端口
TargetPort:               80/TCP
# 当前后端Pod的实际IP和端口列表
Endpoints:                10.244.196.130:80,10.244.140.69:80
# 会话亲和性策略，None表示不保证同一客户端请求到同一Pod
Session Affinity:         None
# 内部流量策略，Cluster表示允许集群内所有节点访问
Internal Traffic Policy:  Cluster
# 与该Service相关的事件记录
Events:                   <none>
```
可以看到 `nginx-deployment-service`  对应的  `Endpoints` 中包含 `pod` 的 `IP`


### 访问测试验证Service是否生效
```bash
# 访问 Service 的 CLUSTER-IP
root@master:~/.kube# curl http://10.98.182.100
<!DOCTYPE html>
xxxxxxxxx
</html>


# 访问 Pod 的 ClusterIP ，同样是该 Service 的 Endpoints
root@master:~/.kube# curl http://10.244.196.130
<!DOCTYPE html>
xxxxxxxxx
</html>

root@master:~/.kube# curl http://10.244.140.69
<!DOCTYPE html>
xxxxxxxxx
</html>
```

### 验证负载均衡

进入两个 pod 分别将其 html 页面内容修改为 `web1` 、 `web2`
```bash
# 进入容器
kubectl exec -it  nginx-deployment-9957b6647-5v2nl -- /bin/sh

# 修改路径如下
# /usr/share/nginx/html/index.html
echo "web1" > index.html
```

随后使用 Service Ip 访问服务

```bash
root@master:~/.kube# curl http://10.98.182.100
web2
root@master:~/.kube# curl http://10.98.182.100
web1
root@master:~/.kube# curl http://10.98.182.100
web2
root@master:~/.kube# curl http://10.98.182.100
web1


root@master:~/.kube# while true;
 do  curl http://10.98.182.100; 
 sleep 1; 
 done; 

web1
web2
web1
web1
web2
web2
web2
web2
web2
web2
```
## Headless 类型

```yaml
apiVersion: v1        # 指定 Kubernetes API 版本，这里是核心 API 组的 v1 版本
kind: Service         # 指定这是一个 Service 对象
metadata:
  name: nginx-deployment-service         # Service 的名称
spec:
  type: ClusterIP      # 集群内部访问的 Service 类型
  clusterIP: None      # None 无头服务（不分配 ClusterIP，直接访问 Pod）
  ports:
    - port: 80      # Service 暴露在集群内部的端口
      targetPort: 80 # 流量将被转发到的后端 Pod 的端口
#      nodePort: 30007 # 在每个节点上暴露的静态端口，用于从集群外部访问 Service
  selector:           # 用于选择哪些 Pod 将被这个 Service 暴露
    app: nginx-deployment        # 标签选择器，匹配具有 app=myapp 标签的 Pod
```

使用 pod 的 ip 进行访问
```bash
root@master:~/.kube# curl http://10.244.196.133
web1
root@master:~/.kube# curl http://10.244.140.72
web2
```


## NodePort 类型

service yaml 文件

```yaml
apiVersion: v1        # 指定 Kubernetes API 版本，这里是核心 API 组的 v1 版本
kind: Service         # 指定这是一个 Service 对象
metadata:
  name: nginx-deployment-service         # Service 的名称
spec:
  type: NodePort      # Service 的类型，NodePort 类型允许从集群外部访问 Service
  ports:
    - port: 80      # Service 暴露在集群内部的端口
      targetPort: 80 # 流量将被转发到的后端 Pod 的端口
      nodePort: 30007 # 在每个节点上暴露的静态端口，用于从集群外部访问 Service
  selector:           # 用于选择哪些 Pod 将被这个 Service 暴露
    app: nginx-deployment        # 标签选择器，匹配具有 app=myapp 标签的 Pod
```

尝试访问
```bash
# 主节点
root@master:~/.kube# while true
> do 
> curl http://192.168.4.101:30007/
> sleep 1
> done;
web2
web2
web1
web1

# 从节点
root@master:~/.kube# while true; do  curl http://192.168.4.102:30007/; sleep 1; done;
web1
web1
web2
web2
web1

# 从节点
root@master:~/.kube# while true; do  curl http://192.168.4.103:30007/; sleep 1; done;
web1
web1
web2
web1
web2
```
发现访问任意节点都有负载均衡效果




## LoadBalancer


![alt text](image-11.png)

### 自建Kubernetes的Loadbalancer类型服务方案-MetaILB


MetalLB 是一款为 Kubernetes 集群提供负载均衡服务的开源工具，专门解决裸金属（Bare-metal）Kubernetes 环境中缺乏原生负载均衡器的问题。在云环境中，Kubernetes 可以直接调用云厂商提供的负载均衡服务（如 AWS ELB、GCP Load Balancer 等），但在物理机、虚拟机等非云环境中，这一功能通常缺失，而 MetalLB 正是为填补这一空白而设计。

#### 核心功能

- 自动分配外部IP地址： 从管理员配置的IP地址池中，为 LoadBalancer 类型服务，分配一个或者多个外部可访问的 IP
- 实现负载均衡： 通过 L2(数据链路层) 或 BGP(边界网关协议) 模式 ，将流量从外部IP转发到集群内的 pod ，实现请求的负载均衡
- 与 Kubernetes 无缝集成： 通过监听 Kubernetes API ， 实时感知服务和端点的变化，动态调整负载均衡规则

#### 工作模式
MetalLB 支持两种主要的工作模式，适用于不同的网络环境：


1. **L2模式(Layer 2 Mode)**
   - 原理： 基于 ARP(地址解析协议，用于IPV4) 或 NDP(邻居发现协议，用于IPV6)，在集群节点中选举一个领导者节点，由该节点负责收发所有负载均衡IP的流量，再通过 Kubernetes 的Service 转发到后端的 Pod
   - 优点： 配置简单，无需额外的网络设备支持，适用于小型集群或网络受限的场景
   - 缺点： 所有流量都会先经过领导者节点，可能称为瓶颈。如果领导者节点故障，需要重新选举，存在短暂中断风险

2. BGP模式(Border Gateway Mode)
   - 原理： 通过 BGP 协议与集群外部的路由器（如数据中心的核心路由器）通信，将负载均衡IP与后端Pod所在节点的路由信息动态同步到路由器。路由器会根据BGP协议学习到的路由将流量直接分发到不同的节点，实现负载均衡
   - 优点： 流量可直接分发到多个节点，路由动态调整，故障恢复快，适用于中大型集群或对性能、可靠性要求较高的场景
   - 缺点： 需要路由器支持BGP协议，配置较为负载，依赖外部网络设备


#### 关键组件

MateILB 由两个主要组件构成：

- Controller： 作为 Deployment 运行（通常为单副本），负责监听 Kubernetes 中 Service 资源的变化，管理 IP地址池（分配、回收），并协调其它组件工作
- Speaker： 作为DaemonSet 运行在每个节点上，负责实现具体的负载均衡逻辑，(L2 模式的 ARP/NDP响应、BGP模式的路由通告)，并确保流量正确转发

#### 使用场景
- 裸金属的Kubernetes集群(无云厂商负责负载均衡服务)
- 需要对外暴露服务，并实现负载均衡的场景
- 替代 NodePort(端口有限)，或 Ingress(依赖反向代理)的外部访问方案，直接通过固定 IP提供服务



####  使用MetaILB实现负载均衡
官方安装教程: <https://metallb.universe.tf/installation/>

1. 添加IPVS支持，在k8s集群中的每个节点上
   
   安装 `ipset` 及 `ipvsadm`
   ```bash
   sudo apt install -y ipset ipvsadm
   ```
   配置ipvsadm模块加载

    ```bash
    cat << EOF | tee  /etc/modules-load.d/ipvs.conf
    ip_vs
    ip_vs_rr
    ip_vs_wrr
    ip_vs_sh
    nf_conntrack
    EOF
    ```
    创建加载模块脚本文件
    ```bash
    cat << EOF | tee ipvs.sh
    #!/bin/sh
    modprobe -- ip_vs
    modprobe -- ip_vs_rr
    modprobe -- ip_vs_wrr
    modprobe -- ip_vs_sh
    modprobe -- nf_conntrack
    EOF
    ```

    给脚本添加执行权限
    ```bash
    chmod +x ipvs.sh
    ```


    执行脚本
    ```bash
    ./ipvs.sh
    ```

    使用 lsmod 查看这几个模块是否开启
    ```bash
    lsmod | grep "ip_"
    ```
2. 修改 kube-proxy 配置，配置 ipvs
   ```bash
   kubectl edit cm kube-proxy -n kube-system
   ```
   在 ConfigMap 里找到mode字段，将其值从""或者"iptables"改成"ipvs" ，

3. MetaILB 部署前准备，需要将 ARP 设置为 严格模式
   ```bash
   kubectl edit cm kube-proxy -n kube-system

   # 编辑配置
   strictARP: true
   ```
3. 部署 MetaILB
   ```bash
   kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.15.2/config/manifests/metallb-native.yaml
   ```
4. 配置
   ```yaml
   # s设置IP池
    apiVersion: metallb.io/v1beta1
    kind: IPAddressPool
    metadata:
      name: first-pool
      namespace: metallb-system
    spec:
      addresses:
        - 192.168.4.10-192.168.4.20
    ---
    # 配置L2模式 - 对IP池进行广播
    apiVersion: metallb.io/v1beta1
    kind: L2Advertisement
    metadata:
      name: example-advertisement
      namespace: metallb-system
    spec:
      ipAddressPools:
        - first-pool  # 关联你定义的 IP 池名称
   ```
5. 部署loadBalancer类型服务
   ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: nginx-deployment        # Deployment名称
      labels:
        app: nginx-deployment
    spec:
      replicas: 2                   # 希望运行的 Pod 副本数量
      selector:
        matchLabels:
          app: nginx-deployment                # 用于选择它管理的 Pod 的标签
      template:
        metadata:
          labels:
            app: nginx-deployment              # Pod 的标签，必须与 selector.matchLabels 匹配
          # Pod的规格说明
        spec:
          containers:
            - name: nginx-container
              #          image: nginx:latest     # 使用的容器镜像
              image: nginx:1.15-alpine
              ports:
                - containerPort: 80     # 容器暴露的端口
    ---
    apiVersion: v1        # 指定 Kubernetes API 版本，这里是核心 API 组的 v1 版本
    kind: Service         # 指定这是一个 Service 对象
    metadata:
      name: nginx-deployment-service         # Service 的名称
    spec:
      type: LoadBalancer      # Service 的类型，NodePort 类型允许从集群外部访问 Service
      ports:
        - port: 8090      # Service 暴露在集群内部的端口
          targetPort: 80 # 流量将被转发到的后端 Pod 的端口
      selector:           # 用于选择哪些 Pod 将被这个 Service 暴露
        app: nginx-deployment        # 标签选择器，匹配具有 app=myapp 标签的 Pod
   ```
6. 测试访问
   
   查看
   ```bash
   root@master:/home/what01# while true 
    > do
    > curl http://192.168.4.10:8090
    > sleep 1
    > done
    web1
    web2
    web1
    web2
   ```

## ExternalName

ExternalName Service核心功能是 **“域名别名”**： 它允许集群内的 Pod 或其它资源通过 Kubernetes 内的的 DNS 名称(例如： `<pod><service-name>.<namespace-name>.svc.cluster.local.`)，访问集群外部的服务，如数据库，第三方API 等，而无需硬编码域名或者IP
### 工作原理
ExternalName Service 通过 kubernetes 集群的 DNS服务实现映射：
1. 当创建 ExternalNameService 时，集群会在DNS 中添加一条 CNAME记录，将 Service 的集群名称（如 my-service.default.svc.cluster.local），指向外部域名（如 external.example.com）
2. 集群内的Pod 访问 Service 时， DNS 会自动解析为 external.example.com 从而访问外部服务
  
### 使用场景

1. 访问外部固定服务，如数据库、对象存储、通过别名隔离外部黄精，方便后期切换外部服务
2. 兼顾内部服务命名规范，统一集群内服务调用的域名格式，如 xxx.svc.cluster.local ,避免硬编码外部域名导致的维护成本
3. 跨命名空间访问： 若 外部服务在其他命名空间，可通过 ExternalName 简化访问路径，如 other-namespace-service。default。svc。cluster.local 映射到跨命名空间域名



### 示例
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-externalname
spec:
  type: ExternalName
  externalName: www.baidu.com

  
```

使用 集群内DNS 解析 Service 的DNS 名称,会发现指向了百度
```bash
dig -t A my-externalname.default.svc.cluster.local. @10.244.140.67

; <<>> DiG 9.18.30-0ubuntu0.22.04.2-Ubuntu <<>> -t a www.baidu.com
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 37504
;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 65494
;; QUESTION SECTION:
;www.baidu.com.                 IN      A

;; ANSWER SECTION:
www.baidu.com.          1032    IN      CNAME   www.a.shifen.com.
www.a.shifen.com.       13      IN      A       183.2.172.17
www.a.shifen.com.       13      IN      A       183.2.172.177

;; Query time: 47 msec
;; SERVER: 127.0.0.53#53(127.0.0.53) (UDP)
;; WHEN: Mon Aug 04 09:18:49 CST 2025
;; MSG SIZE  rcvd: 101
```

### 注意事项

1. 外部域名必须是FQDN： 必须是完整域名 如 xxx.com 不能是IP地址
2. 无选择器和端口配置： ExternalName Service 不支持 spec.selector spec.port 等字段，配置会被忽略
3. 依赖集群 DNS 
4. 不支持TCp端口映射： 如果外部服务需要指定端口，如 xxx。com:3306 ，需要在 Pod 访问时手动添加端口 如： external-db-service:3306 ，Service 本身不处理端口


### 扩展：与Endpoint配合使用-映射IP场景
```yaml
# 1. 创建无选择器的 Service
apiVersion: v1
kind: Service
metadata:
  name: external-ip-service
spec:
  ports:
    - protocol: TCP
      port: 80        # 集群内访问的端口
      targetPort: 8080  # 外部服务的端口

---
# 2. 手动创建 Endpoints，关联外部 IP
apiVersion: v1
kind: Endpoints
metadata:
  name: external-ip-service  # 必须与 Service 同名
subsets:
  - addresses:
      - ip: 192.168.1.100  # 外部服务的 IP 地址
    ports:
      - port: 8080  # 外部服务的端口（需与 Service 的 targetPort 一致）
```
此时，集群内 Pod 访问 external-ip-service:80 会被转发到 192.168.1.100:8080。


### 不同的命名空间访问

1. 通过完整 DNS 名称访问
   ```bash
   # 示例：访问命名空间「prod」中的「api-service」服务
    curl http://api-service.prod.svc.cluster.local:8080
   ```
   在应用配置中（如配置文件、环境变量），也可以直接使用该格式：
   ```yaml
   # 例如在 Deployment 的环境变量中配置
    env:
      - name: API_URL
        value: "http://api-service.prod.svc.cluster.local:8080"
   ```
2. 通过简化的 DNS 名称访问.若省略 .svc.cluster.local 后缀，Kubernetes DNS 仍能解析（自动补全后缀）：
    ```bash
    # 简化写法（等效于完整 DNS）
     curl http://api-service.prod:8080
    ```
3. 如果需要频繁访问其他命名空间的服务，可在当前命名空间创建一个 ExternalName Service 作为 “别名”，简化访问路径：
   ```yaml
   # 在当前命名空间（如 dev）创建别名 Service
  apiVersion: v1
  kind: Service
  metadata:
    name: prod-api-alias  # 本地别名
    namespace: dev
  spec:
    type: ExternalName
    externalName: api-service.prod.svc.cluster.local  # 目标服务的完整 DNS
   ```
   创建后，当前命名空间的 Pod 可直接通过别名访问：
   ```bash
   curl http://prod-api-alias:8080  # 无需指定命名空间
   ```
**网络策略（NetworkPolicy）的影响**

默认情况下，Kubernetes 允许所有命名空间之间的网络通信。但如果集群中配置了 NetworkPolicy，则需要显式授权跨命名空间访问规则，否则会被拦截。


示例：允许命名空间 A 访问命名空间 B 的服务
```yaml
# 在命名空间 B 中创建 NetworkPolicy，允许 A 访问
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-namespace-a
  namespace: B  # 目标命名空间
spec:
  podSelector:
    matchLabels:
      app: target-service  # 被访问的服务标签
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: A  # 允许来源命名空间 A
```


# SessionAffinity

默认情况下，Kubernetes Service 会采用 轮训（Round Robin），或者随机策略将请求分发到后端Pod，但对于以下场景，需要通过 SessionAffinity 来确保请求定向到固定的Pod。


- 应用使用**本地会话存储**（如未共享的内存、文件）
- 依赖 **连接上下文** （如长连接、临时缓存）
- 需要避免 **分布式事务冲突** 的场景

## SessionAffinity类型

1. None 默认
   
   不启用会话亲和性，按照负载均衡策略分发到后端Pod

   适合 无状态服务


1. ClientIP
   
   基于客户端IP绑定会话

   - 同一客户端 IP 的请求会被路由到 同一 pod
   - 如果目标 Pod 故障，请求会被分配到其它可用 Pod 并重新绑定新的pod


## 示例
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: my-app
  ports:
  - port: 80
    targetPort: 8080
  sessionAffinity: ClientIP  # 启用 ClientIP 亲和性
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800  # 会话超时时间（默认 10800 秒 = 3 小时）
```

## 适用场景与局限性


**适用场景**
- 依赖本地会话，如未使用分布式缓存的会话
- 需减少跨 Pod 数据同步的场景，如本地计算缓存
- 简单的负载均衡优化，避免频繁切换 Pod 导致的资源浪费

**局限性**

1. 仅基于客户端IP绑定
   - 若多个客户端共享同一出口IP（如NAT网关），会被认为是同一个客户端，导致请求集中到单个pod引发负载不均衡
   - 客户端IP变化，会导致会话丢失

1. 不支持更复杂的亲和策略
   
   - 无法基于 HTTP 会话ID（如Cookie） 绑定（需依赖Ingress，如 Nginx Ingress 的 SessionAffinity 配置）
   - 不支持基于用户、请求路径等自定义维度的亲和性

1. 故障转移依赖Pod可用性
   
   若绑定的 Pod 故障，请求会自动切换到其它 pod，但原有会话状态（如本地缓存）会丢失，需应用自身处理（如容错机制）


## 与Ingress会话亲和性的区别


Service 的 SessionAffinity 是 `四层（TCP/UDP）` 负载均衡的亲和性，仅基于IP：

而 Igress 控制器（如Nginx Ingress）支持七层（HTTP/HTTPS） 会话亲和性，可基于 Cookie 、 HTTP 头信息等绑定会话、更灵活。

例如，Nginx Ingress 配置 Cookie 亲和性
```yaml
# 使用 networking.k8s.io/v1 API 版本，这是当前 Ingress 的稳定版本
apiVersion: networking.k8s.io/v1
# 资源类型为 Ingress，用于管理外部访问集群内服务的规则
kind: Ingress

metadata:
  # Ingress 资源的名称，在命名空间内唯一
  name: my-ingress
  # 注解(annotations)：用于配置特定 Ingress 控制器的额外参数
  annotations:
    # 配置会话亲和性为基于 Cookie 的方式
    # 这会确保来自同一客户端的请求始终路由到同一后端 Pod
    nginx.ingress.kubernetes.io/affinity: "cookie"
    # 指定用于会话亲和性的 Cookie 名称
    nginx.ingress.kubernetes.io/session-cookie-name: "INGRESSCOOKIE"
    # 设置 Cookie 的过期时间，单位为秒（此处 172800 秒 = 48 小时）
    nginx.ingress.kubernetes.io/session-cookie-expires: "172800"

spec:
  # 定义访问规则的集合
  rules:
  - host: example.com  # 匹配的域名，只有访问该域名的请求才会应用此规则
    http:  # HTTP 协议的路由规则
      paths:  # 路径匹配规则列表
      - path: /  # 匹配的 URL 路径，此处匹配所有路径（根路径及子路径）
        pathType: Prefix  # 路径匹配类型为前缀匹配
        backend:  # 匹配成功后转发到的后端服务
          service:
            name: my-service  # 目标服务的名称
            port:
              number: 80  # 目标服务的端口号
```

# Service 的 DNS 解析

DNS 服务监视 kubernetes API ，为每一个 Service 创建 DNS 记录用户域名解析。

其 headless 需要使用域名来解决访问问题

DNS 记录格式为
```bash
<service-name>.<namespace-name>.svc.cluster.local.
```

## 通过 kubernetes 的 DNS pod 解析 Service 域名

首先需要找到 DNS pod 的地址

```bash
root@master:~/.kube# kubectl get pods -n kube-system -o wide
NAME                                       READY   STATUS    RESTARTS       AGE    IP              NODE     NOMINATED NODE   READINESS GATES
。。。
coredns-6f6b679f8f-64n7l                   1/1     Running   0              4d1h   10.244.140.67   node02   <none>           <none>
coredns-6f6b679f8f-xgxjc                   1/1     Running   0              4d1h   10.244.140.66   node02   <none>           <none>
。。。
```

随后使用 dig 命令解析域名
```bash
root@master:~/.kube# dig -t A  nginx-deployment-service.default.svc.cluster.local. @10.244.140.67

; <<>> DiG 9.18.30-0ubuntu0.22.04.2-Ubuntu <<>> -t A nginx-deployment-service.default.svc.cluster.local. @10.244.140.67
;; global options: +cmd
;; Got answer:
;; WARNING: .local is reserved for Multicast DNS
;; You are currently testing what happens when an mDNS query is leaked to DNS
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 14272
;; flags: qr aa rd; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1
;; WARNING: recursion requested but not available

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 1232
; COOKIE: 2b8e6d05e7e63888 (echoed)
;; QUESTION SECTION:
;nginx-deployment-service.default.svc.cluster.local. IN A

;; ANSWER SECTION:
nginx-deployment-service.default.svc.cluster.local. 30 IN A 10.244.140.72
nginx-deployment-service.default.svc.cluster.local. 30 IN A 10.244.196.133

;; Query time: 0 msec
;; SERVER: 10.244.140.67#53(10.244.140.67) (UDP)
;; WHEN: Tue Jul 29 17:15:05 CST 2025
;; MSG SIZE  rcvd: 223
```








