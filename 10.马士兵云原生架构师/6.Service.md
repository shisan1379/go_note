

# Service的作用

使用 kubernetes 集群运行工作负载时，由于 Pod 经常处于用后即焚的状态， Pod 经常被重新生成，因此Pod 对应的地址也会经常变化，导致无法直接访问 Pod 提供的服务。 kubernetes 使用 service 来解决这一问题。 即在 Pod 前面使用 Service 对 Pod 进行代理，无论 Pod 怎样变化，只要有 Label ，就可以让 Service 联系上 Pod。 把 Pod Ip 地址 添加到 Service 对应的端点列表（Endpoints） 实现对PodIP跟踪，进而实现通过 Service 访问 Pod 目的。



- 通过 Service 为 Pod 客户端提供访问 Pod 的方法
- 通过标签动态感知 Pod IP 地址变化等
- 防止 Pod 失联
- 定义 Pod 访问策略
- 通过 label-selector 相关联
- 通过 Service 实现 Pod 的负载均衡 （TCP/UDP 4层）
- 底层实现由 kube-proxy 通过 userspace 、 iptables 、 ipvs 三种代理模式


# kube-proxy的三种代理模式


- kubernetes 集群中有三层网络
  - 一类是真实的，例如 Node Network ， Pod Network 提供真实IP地址
  - 一类是虚拟的， 例如 Cluster Network 、 Service Network，不会出现在接口上，仅会出现在 Service 中

- kube-proxy 始终监控（watch） kube-apiserver 上关于Service相关的资源变动状态，一旦获取相关信息， kube-proxy 都要把相关信息转换为当前节点上能够实现Service资源调度的规则，进而实现访问Service就能够获取Pod所提供的服务

- kube-proxy 三种代理模式， UserSpace 模式 、 iptables 模式、 ipvs模式



## UserSpace 模式

UserSpace 是 kube-proxy 最早的实现模式，基于用户空间进程完成流量转发，目前已被弃用（K8s v1.20 后完全移除）。

### 工作原理


**流量路径**

1. kube-proxy 会为每个 Service 随机监听一个端口 proxy-port ，并增加一条 iptables 规则。
2. 客户端请求，先通过 iptables 规则重定向到 用户空间的 kube-proxy 进程（就是发送到 proxy-port），再由 kube-proxy 根据负载均衡算法（如轮询）转发到后端 pod
3. 响应流量原路返回（先到 kube-proxy，再经 iptables 回传给客户端）
```
客户端 → 内核 iptables（重定向） → 用户空间 kube-proxy 进程 → 后端 Pod
后端 Pod 响应 → kube-proxy 进程 → 内核 iptables → 客户端
```



**核心逻辑**

kube-proxy 进程持续监听 API Server ，同步 Service 和 Endpoint 变化，并动态调整 iptables 规则 和自身的转发策略



### 特点

**优点**
1. 兼容性强，不依赖特定内核功能，支持老旧 Linux 发行版
2. 用户空间实现负载均衡算法
3. 调试简单，流量在用户空间可见，可以通过 netstat 等工具直接监控


**缺点**
1. 性能极差，每次转发需经过用户空间和内核空间的两次切换，延迟高，吞吐量低
2. 单点风险， kube-proxy 进程崩溃会导致 节点上的 service 失效
3. 资源消耗高， 用户空间进程处理大量连接时占用 CPU/内存较高

## iptables 模式

iptables 模式是 Kubernetes v1.8 后的默认模式（替代 UserSpace），基于 Linux 内核的 iptables 规则实现负载均衡，直接在内核空间处理流量。


### 工作原理

1. kube-proxy 监听 API Server 中 Service 和 Endpoint 的变化，动态生成 iptables 规则（存储在内核中）
2. 客户端请求直接通过内核的iptables规则匹配并转发到后端pod，无需经过用户空间

```
客户端 → 内核 iptables 规则（直接转发） → 后端 Pod
后端 Pod 响应 → 内核 iptables 规则 → 客户端
```


### 特点


**优点**
1. 完全是基于内核空间的iptables转发，避免用户空间切换开销
2. 部署简单，大多数 Linux 都支持 iptables ，无需额外配置

**缺点**
1. 规则匹配效率低， uptables 规则是 线性匹配（类似链表），当 Service 数量庞大时（数千个），规则匹配延迟会显著增加
2. 功能有限： 不支持高级的负载均衡算法（如加权轮询，最少连接等），也不支持会话保持（需要额外配置）
3. 故障恢复慢： 当后端 pod 故障时，需要依赖 kube-proxy 重新生成 iptables 规则，恢复延迟较高




## IPVS 模式
IPVS（IP Virtual Server）是 Linux 内核的负载均衡模块，专为大规模服务设计。Kubernetes v1.8 引入 IPVS 模式作为 iptables 的高性能替代方案，需手动启用。



### IPVS模式的核心机制

IPVS模式下 kube-proxy 会
1. 通过 ipvsadm 配置 IPVS 规则
   - 根据 Service 创建 虚拟服务(Virtual Service), 对应 ClusterIp 和 端口
   - 根号有 Endpoints 创建 真实服务器（Real Server），对应后端 Pod 的 Id 和端口
   - 配置负载均衡算法（如轮询、最少连接数）
2. IPVS 直接在内核层转发流量
   - 客户请求到达节点后，IPVS 直接将流量转发至后端 Pod（基于IPVS规则），无需经过用户空间



### IPVS模式下 iptables 的使用场景

尽管 IPVS 负责主要的负载均衡，但以下场景仍然依赖iptables

1. 网络策略层（Network Policy）
   - 功能： 控制 Pod 间的流量访问规则（如允许、拒绝特定 Pod 通信）
   - 实现： 完全依赖 iptables 规则，与 IPVS 无关
   - 示例: 此策略会生成 iptables规则，限制只有带 role=frontend 标签的 Pod 能访问 role=db 的 Pod。
    ```yaml
    apiVersion: networking.k8s.io/v1
    kind: NetworkPolicy
    spec:
      podSelector:
        matchLabels:
          role: db
      ingress:
        - from:
            - podSelector:
                matchLabels:
                  role: frontend
    ```

2. SNAT（源地址转换）
   - 功能： 处理 IPVS 未覆盖的边缘场景（如HostPort、NodePort、ExternalIp 等）
   - 实现： 通过 iptables 规则将流量导向 IPVS 或直接转发
     ```bash
      -A POSTROUTING ! -d 10.0.0.0/16 -m comment --comment "kubernetes postrouting rules" -j MASQUERADE
     ```







### 工作原理







**流量路径**

1. kube-proxy 启动时会检查并加载 ip_vs 模块，然后根据 Service 和 Endpoint 创建 IPVS 虚拟服务器（Virtual Server 对应 Service 的 Cluster IP） 和后端真实服务器（Real Server 对应的 Pod IP）; 客户端请求直接由 IPVS 在内核空间转发到后端 Pod

```
客户端 → 内核 IPVS 规则（直接转发） → 后端 Pod
后端 Pod 响应 → 内核 IPVS 规则 → 客户端
```



### 特点


**优点**
1. 性能最优，内核级转发，哈希表查询，支持百万并发连接，接近于硬件负载均衡，
2. 支持大规模集群： 规则查询效率高，适合数千个甚至数万个 Service 的场景
3. 功能丰富： 支持多种负载均衡算法和会话保持
4. 故障恢复快： 内置健康检查，后端 Pod 故障可快速移除


**缺点**
1. 依赖内核模块： 需要Linux 启动 ip_vs 模块，通常需要手动加载
2. 配置复杂： 需要提前安装 ipset 和 ipvsadm工具， 且需要内核支持


## 三种模式对比


| **对比维度**         | **UserSpace 模式**                | **iptables 模式**                  | **IPVS 模式**                      |
|----------------------|-----------------------------------|-----------------------------------|-----------------------------------|
| **工作空间**         | 用户空间（kube-proxy 进程）       | 内核空间（iptables 规则）          | 内核空间（IPVS 模块）              |
| **转发性能**         | 最低（两次上下文切换）            | 中（线性规则匹配）                | 最高（哈希表查询）                |
| **负载均衡算法**     | 基础（轮询）                      | 有限（随机、轮询）                | 丰富（RR、WRR、LC、SH 等）        |
| **规则查询效率**     | 低（用户空间处理）                | 中（线性遍历）                    | 高（O(1) 哈希查询）               |
| **大规模支持**       | 差（性能瓶颈明显）                | 一般（数千 Service 可能卡顿）      | 优（数万 Service 无压力）          |
| **故障恢复速度**     | 慢（依赖进程重启）                | 中（依赖 kube-proxy 更新规则）    | 快（内置健康检查）                |
| **依赖组件**         | iptables（仅重定向）              | iptables、ipset                   | ip_vs 内核模块、ipset、ipvsadm    |
| **适用场景**         | 遗留环境、调试                    | 中小规模集群（默认选择）          | 大规模集群、高性能需求            |
| **K8s 支持状态**     | 已弃用（v1.20+ 移除）             | 稳定（默认模式）                  | 稳定（推荐大规模场景）            |
| **典型延迟（请求）** | 数百微秒（甚至毫秒级）            | 数十微秒                          | 数微秒                            |










