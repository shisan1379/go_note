
# cgroups

cgroups（全称：control groups）是 Linux 内核的一个功能，它可以实现限制进程或者进程组的资源（如 CPU、内存、磁盘 IO 等）。

> 在 2006 年，Google 的工程师（ Rohit Seth 和 Paul Menage 为主要发起人） 发起了这个项目，起初项目名称并不是cgroups，而被称为进程容器（process containers）。在 2007 年cgroups代码计划合入Linux 内核，但是当时在 Linux 内核中，容器（container）这个词被广泛使用，并且拥有不同的含义。为了避免命名混乱和歧义，进程容器被重名为cgroups，并在 2008 年成功合入 Linux 2.6.24 版本中。cgroups目前已经成为 systemd、Docker、Linux Containers（LXC） 等技术的基础。


# cgroups 功能及核心概念



cgroups 主要提供了如下功能。

- 资源限制： 限制资源的使用量，例如我们可以通过限制某个业务的内存上限，从而保护主机其他业务的安全运行。

- 优先级控制：不同的组可以有不同的资源（ CPU 、磁盘 IO 等）使用优先级。

- 审计：计算控制组的资源使用情况。

- 控制：控制进程的挂起或恢复。


cgroups功能的实现依赖于三个核心概念：子系统、控制组、层级树。

- **子系统（subsystem）**：是一个内核的组件，一个子系统代表一类**资源调度控制器**。例如内存子系统可以限制内存的使用量，CPU 子系统可以限制 CPU 的使用时间。
  
  可以使用 `lssubsys -a` 查看内核支持的 subsystem
    ```bash
    what@what-virtual-machine:~$ lssubsys -a
    cpuset
    cpu
    cpuacct
    blkio
    memory
    devices
    freezer
    net_cls
    perf_event
    net_prio
    hugetlb
    pids
    rdma
    misc
    ```

  常用的子系统如下
  - cpu：限制 CPU 使用时间。
  - memory：限制内存使用量。
  - blkio：限制块设备的 I/O。
  - cpuset：限制进程可使用的 CPU 和内存节点。
  - devices：控制进程对设备的访问。
  - freezer：冻结和恢复进程。

  在 Docker 中，主要使用的 cgroups 子系统包括 cpu、memory 和 blkio，用于管理容器的资源限制和性能优化。通过这些子系统，Docker 可以确保每个容器不会过度消耗主机资源，从而提高系统的稳定性和效率。


- **控制组（cgroup）**：cgroups 是 control groups 的缩写。
  
  表示一组进程和一组带有参数的子系统的关联关系。例如，一个进程使用了 CPU 子系统来限制 CPU 的使用时间，则这个进程和 CPU 子系统的关联关系称为控制组。
  
  一个进程可以加入某个 cgroup，也可以从某个 cgroup 迁移到另外一个 cgroup

- **层级树（hierarchy）**：是由一系列的控制组按照树状结构排列组成的。这种排列方式可以使得控制组拥有父子关系，子控制组默认拥有父控制组的属性，也就是子控制组会继承于父控制组。
  比如，系统中定义了一个控制组 c1，限制了 CPU 可以使用 1 核，然后另外一个控制组 c2 想实现既限制 CPU 使用 1 核，同时限制内存使用 2G，那么 c2 就可以直接继承 c1，无须重复定义 CPU 限制。
  
  可以使用 `mount | grep cgroup` 命令查看


cgroups 的三个核心概念中，子系统是最核心的概念，因为子系统是真正实现某类资源的限制的基础。


本地内存释义：

> 本地内存是指与特定处理器直接相连的内存。每个处理器在 NUMA（非统一内存访问）架构中通常有自己的本地内存节点，这些内存是为该处理器设计的，访问速度最快。当处理器访问其本地内存时，延迟最低，带宽最大。
> 与此相对的是“远程内存”，即一个处理器访问其他处理器的内存节点时所使用的内存。由于涉及跨节点的数据传输，这种访问通常会引入更高的延迟和更低的带宽。因此，在优化系统性能时，通常会尽量让处理器访问本地内存，以减少延迟和提高效率。

# 版本区别

1. 层次结构（Hierarchy）
    - cgroup v1：每个资源控制器（如CPU、内存等）都有独立的层次结构，意味着可以为不同的控制器定义不同的cgroup层次。不同资源控制器（subsystem）彼此独立，使用不同的cgroup树。
  
    - cgroup v2：所有资源控制器共享一个单一的统一层次结构。这样有助于简化管理，使资源控制和进程管理更加一致。
3. 资源分配和限制
    - cgroup v1：可以为每个资源控制器独立设置不同的资源限制，允许多个资源控制器并行工作。然而，这样容易导致层次结构之间的资源分配冲突，进而引发不一致的行为。
  
    - cgroup v2：通过统一的层次结构提供更精确和一致的资源分配。cgroup v2对资源调度更加智能，例如，内存和CPU资源之间可以更好地协调，避免资源调度冲突。
  
4. 支持的控制器
    - cgroup v1：每个控制器是独立实现的，不同控制器有各自的特性和功能，有时这些功能和控制方式会略有不同。所有控制器的行为并不统一。
  
    - cgroup v2：控制器的设计和功能在cgroup v2中更加统一，提供了更一致的行为，并且一些控制器只在cgroup v2中得到支持或优化。
5. 内存管理
    - cgroup v1：内存控制器的分层方式较复杂，在父子cgroup之间可能会产生不一致的问题，尤其是在处理缓存时。
  
    - cgroup v2：内存控制在v2中得到了简化和增强，改善了内存管理的一致性和隔离性。对内存压力的管理更加平滑，也更容易理解。
6. IO控制
    - cgroup v1：I/O限制和优先级的管理较为复杂，涉及多个控制器和机制。
    - cgroup v2：I/O控制得到了极大改进，简化了配置和管理，提供了更高效的I/O调度和更精确的限制。

## 如何查看当前系统使用的是cgroup v1还是v2

可以通过以下命令查看系统当前使用的 `cgroup` 版本：

```bash
what@what-virtual-machine:/sys/fs/cgroup/mygroup$ mount | grep cgroup
cgroup2 on /sys/fs/cgroup type cgroup2 (rw,nosuid,nodev,noexec,relatime,nsdelegate,memory_recursiveprot)
cgroup-test on /home/what/cgroup-test type cgroup (rw,relatime,name=cgroup-test)
```

或者

```bash
cat /sys/fs/cgroup/cgroup.controllers
```

如果有输出，说明正在使用 `cgroup v2`

```bash
cpuset cpu io memory hugetlb pids rdma misc
```


# CPU 子系统
`cpu子系统` 通过 `CFS` （Completely Fair Scheduler，完全公平调度器）和 `RT` （Real-Time，实时调度器）两个调度程序来实现对 `CPU` 使用的管理。

- **CFS调度器** ：根据进程优先级/权重或 `cgroup` 来分配 `CPU` 时间。它模拟了一个完全理想的多任务处理器，确保所有进程都能公平地获得 `CPU` 时间。 `CFS` 调度器使用红黑树来管理可运行状态的进程，根据进程的虚拟运行时间进行调度。
  
- **RT调度器** ：对实时进程使用 `CPU` 的时间进行限定，适用于需要实时响应的应用场景。





## cgroup1 


在 cgroup1 版本中，配置位于不同层级，但均位于 `/sys/fs/cgroup/` 目录下，子系统分别处于不同的文件夹，要新建 cgroup ，需要在不同的子系统(例如 cpu、memory)下创建文件夹,新建的文件夹中会自动生成相关配置文件,下面介绍以下文件重点文件已加粗：

1. cgroup.clone_children:

  - 用途: 控制是否让该控制组的子进程共享父进程的资源。
  - 格式: 0（不共享）或 1（共享）。
  ```bash
  echo 1 > cgroup.clone_children
  ```

1. **cgroup.procs:**
   
  - 用途: 用于将进程添加到该控制组或列出当前组内的进程。
  - 格式: 直接写入进程ID。
  ```bash
  echo <PID> > cgroup.procs
  ```
1. cpu.cfs_burst_us:
  - 用途: 设置在CFS调度器下，允许的CPU突发时间，影响短期负载能力。
  - 格式: 微秒数（整数）。
  ```bash
  echo 20000 > cpu.cfs_burst_us
  ```

1. cpu.cfs_period_us:
  - 用途: 定义CFS调度周期，决定分配CPU时间的基本周期。
  - 格式: 微秒数（整数）。
  ```bash
  echo 100000 > cpu.cfs_period_us
  ```
1. **cpu.cfs_quota_us:**
  - 用途: 在一个调度周期内，限制CPU可使用的最大时间。
  - 格式: 微秒数（整数）。
  ```bash
  echo 50000 > cpu.cfs_quota_us
  ```

1. cpu.idle:
   
  - 用途: 提供CPU闲置时间的统计信息，供监控使用。
  - 格式: 只读，不可修改。

1. cpu.rt_period_us:
  - 用途: 设置实时任务的调度周期，影响实时任务的调度频率。
  - 格式: 微秒数（整数）。
  ```bash
  echo 100000 > cpu.rt_period_us
  ```

1. cpu.rt_runtime_us:
  - 用途: 定义实时任务在一个周期内允许的最大运行时间。
  - 格式: 微秒数（整数）。
  ```bash
  echo 95000 > cpu.rt_runtime_us
  ```
1. cpu.shares:
  - 用途: 设置该控制组的CPU资源权重，相对其他控制组的使用权重。
  - 格式: 整数（默认1024）。
  ```bash
  echo 512 > cpu.shares
  ```
1. cpu.stat:

  - 用途: 显示CPU的使用统计信息，包括已分配的时间和使用的时间。
  - 格式: 只读，不可修改。

1. notify_on_release:

  - 用途: 控制当释放控制组时是否发出通知。
  - 格式: 0（不通知）或 1（通知）。
  ```bash
  echo 1 > notify_on_release
  ```
1. tasks:

  - 用途: 列出当前控制组中的所有进程ID，便于监控和管理。
  - 格式: 只读，输出进程ID。

## cgroup2 

在 cgroup2 版本中，所有的配置处于同一个层级，均位于 `/sys/fs/cgroup/` 目录下，要新建 cgroup ，在该目录下，创建文件夹即可，新建的文件夹中会自动生成相关配置文件,下面介绍以下文件重点文件已加粗：


1. **cpu.max**
  - 用途-设置：设置每个调度周期内允许使用的最大CPU时间。
  - 格式：`<max> <period>`，例如 50000 100000 表示在每100毫秒内最多使用50毫秒的CPU时间。
  ```bash
  echo "50000 100000" | sudo tee /sys/fs/cgroup/mygroup/cpu.max
  ```
  这条命令表示在每100毫秒的周期内，最多允许该cgroup使用50毫秒的CPU时间。

1. **cpu.max.burst**
   
  - 用途-设置：允许在短时间内超过cpu.max限制的额外CPU使用量。用于处理瞬时负载。
  - 设置：该值通常用于结合cpu.max使用，以提高短期性能。
  ```bash
  echo "10000" | sudo tee /sys/fs/cgroup/mygroup/cpu.max.burst
  ```
  此设置允许该cgroup在短期内额外使用10毫秒的CPU时间，以便应对瞬时负载。


2. cpu.pressure
  - 用途-读取：提供CPU压力的监控信息。它能反映在过去的一段时间内，CPU资源是否紧张。
  - 读取：可以读取该文件以获取当前的CPU压力状况。
  ```bash
  cat /sys/fs/cgroup/mygroup/cpu.pressure
  ```
  读取此文件可以查看CPU资源的压力状况，包括等待和被调度的时间。
  
3. **cpuset.cpus**
  - 用途-设置：定义允许该cgroup使用的CPU核心列表。
  - 示例：设置为0-3表示允许使用第0到第3个CPU核心。
  ```bash
  echo "0-3" | sudo tee /sys/fs/cgroup/mygroup/cpuset.cpus
  ```
  该命令允许mygroup使用第0到第3个CPU核心。

4. cpuset.cpus.effective
  - 用途-读取：显示当前cgroup实际能够使用的CPU核心列表。它考虑了父cgroup的限制。
  - 读取：只读文件，提供有效的CPU核心信息。
  ```bash
  cat /sys/fs/cgroup/mygroup/cpuset.cpus.effective
  ```
  此文件会显示mygroup当前有效的CPU核心，可能受到父cgroup限制的影响。

5. cpuset.cpus.exclusive
  - 用途：指示该cgroup是否在使用的CPU核心上独占运行。设置为1表示独占，0表示共享。
  - 设置：允许在负载较高时确保该cgroup的核心不会被其他进程干扰。
  ```bash
  echo "1" | sudo tee /sys/fs/cgroup/mygroup/cpuset.cpus.exclusive
  ```
  设置为1表示mygroup在指定的CPU核心上独占运行，不允许其他进程使用这些核心。

6. cpuset.cpus.exclusive.effective
  - 用途-读取：显示当前cgroup是否独占使用的CPU核心的有效状态。
  - 读取：只读文件，提供有效的独占信息。
  ```bash
  cat /sys/fs/cgroup/mygroup/cpuset.cpus.exclusive.effective
  ```
  提供当前mygroup的独占状态，表明该cgroup是否真正拥有其指定的CPU核心。
  
7. cpuset.cpus.partition
  - 用途：在分区模式下，定义此cgroup的CPU核心分区。
  - 设置：用于管理更复杂的CPU核心分配。
  ```bash
  echo "0-3" | sudo tee /sys/fs/cgroup/mygroup/cpuset.cpus.partition
  ```
  用于设置在复杂的CPU核心分配策略下，mygroup可使用的核心分区。
  
8. cpuset.mems
  - 用途：定义该cgroup可以使用的内存节点列表（NUMA节点）。
  - 示例：设置为0表示只使用内存节点0。
  ```bash
  echo "0" | sudo tee /sys/fs/cgroup/mygroup/cpuset.mems
  ```
  允许mygroup使用内存节点0，适用于多节点系统的资源管理。
  
9.  cpuset.mems.effective
  - 用途：显示当前cgroup实际能够使用的内存节点列表。
  - 读取：只读文件，提供有效的内存节点信息。
  ```bash
  cat /sys/fs/cgroup/mygroup/cpuset.mems.effective0
  ```
  输出当前mygroup能够有效使用的内存节点，可能受父cgroup限制影响。

10. cpu.stat
  - 用途：提供该cgroup的CPU使用统计信息，包括运行时间、调度次数等。
  - 读取：定期读取以监控资源使用情况。
  ```bash
  cat /sys/fs/cgroup/mygroup/cpu.stat
  ```
  显示mygroup的CPU运行时间、调度次数等统计数据。


11. cpu.stat.local
  - 用途：与cpu.stat类似，但只包括该cgroup内进程的统计信息。
  - 读取：用于更精细的资源监控。
  ```bash
  cat /sys/fs/cgroup/mygroup/cpu.stat.local
  ```
  提供更精确的统计，专注于该cgroup内部进程的CPU使用情况。

12.  cpu.uclamp.max
  - 用途：设置最大CPU调度优先级，控制高优先级的资源使用。
  - 设置：影响调度器如何调度高优先级任务。
  ```bash
  echo "100" | sudo tee /sys/fs/cgroup/mygroup/cpu.uclamp.max
  ```
  确保高优先级的任务能够获得足够的CPU资源。



13.  cpu.uclamp.min
  - 用途：设置最小CPU调度优先级，确保任务至少获得的资源。
  - 设置：影响调度器对低优先级任务的调度。
  ```bash
  echo "50" | sudo tee /sys/fs/cgroup/mygroup/cpu.uclamp.min
  ```
  可以用来保证低优先级任务在调度时至少获得一定的CPU资源。


14.  cpu.weight
  - 用途：设置cgroup的CPU权重值，影响调度器的资源分配。
  - 示例：值越高，获得的CPU资源越多，默认值为100。
  ```bash
  echo "200" | sudo tee /sys/fs/cgroup/mygroup/cpu.weight
  ```


15.  cpu.weight.nice
  - 用途：调整CPU权重的“nice”值，使其在调度时更具灵活性。
  - 设置：可以根据任务的优先级进行调整，影响调度器的选择。
  ```bash
  echo "10" | sudo tee /sys/fs/cgroup/mygroup/cpu.weight.nice
  ```
  通过调整nice值，可以在调度时影响任务的优先级，帮助管理高负载环境下的资源分配。


## 使用示例：以group2为例
1. 创建 cgroup
   
  首先，你需要创建一个新的cgroup目录。在cgroup v2中，所有的资源控制器（包括CPU）都是在统一的挂载点下管理的。

  ```bash
  sudo mkdir /sys/fs/cgroup/my_cgroup
  cd /sys/fs/cgroup/my_cgroup
  ```
  这条命令会在/sys/fs/cgroup下创建一个名为my_cgroup的目录,同时生成相应的配置文件。

1. 设置 CPU 使用限制
   
   使用cpu.max文件来设置CPU使用限制。格式为`<max> <period>`，其中：

   - `<max>` ：每个周期内允许使用的最大CPU时间（微秒）。
   - `<period>` ：周期的长度（微秒）。
  
  例如，如果你想限制该cgroup在每100毫秒内最多使用50毫秒的CPU时间，可以执行以下命令：
  ```bash
  echo "50000 100000" | sudo tee cpu.max
  ```
  `50000` 表示在每个 `100000` 微秒（`100`毫秒）的周期中，最多使用`50000`微秒（50毫秒）的CPU时间。

1. 将当前 `bash` 进程添加到 cgroup

   ```bash
   echo $$ | sudo tee cgroup.procs
   ```
2. 查看当前进程ID
   ```bash
   echo $$
   ```
3. 执行死循环跑满，当前进程
   ```bash
   while true; do echo; done;
   ```
4. 打开新的窗口，使用 top 查看该进程的CPU占用率
   ```bash
    top -p 3485
   ```
   ![alt text](image-21.png)


# 内存子系统

## cgroup1

### cgroup 提供的功能

1. 内存限制
  - 硬性限制（memory.limit_in_bytes）：这是一个强制性的内存限制，指定了 cgroup 可以使用的最大内存量。如果进程使用的内存超出该值，内核会尝试释放内存，或者直接触发 OOM（Out Of Memory）终止进程。
  - 软性限制（memory.soft_limit_in_bytes）：这个限制是一个较为宽松的限制，系统在资源紧张时会优先回收超过软性限制的内存，但不会强制终止进程。

3. 交换限制
   - memory.memsw.limit_in_bytes：用于限制内存和交换空间的总和。如果这个限制被超出，进程可能会被 OOM 终止。
  
   - memory.swappiness：这个值控制 cgroup 中内存和交换空间的使用优先级，范围是 0 到 100。较低的值意味着更少的交换使用，0 表示禁止使用 swap，100 表示最大化使用 swap。
   
4. OOM行为
   memory.oom_control：允许启用或禁用 OOM 杀手。通过设置，可以控制是否允许内核在内存不足时终止进程。
   
5. 统计信息

  内存子系统提供了关于内存使用的详细统计信息，帮助管理员监控资源使用：

  - memory.usage_in_bytes：表示当前 cgroup 使用的内存量。
  - memory.max_usage_in_bytes：显示自创建 cgroup 以来，所使用的最大内存量。
  - memory.failcnt：统计超过内存限制的次数。
  - memory.stat：包含更多详细的内存使用统计，包括匿名内存、文件缓存等。

6. 文件缓存
  内存子系统支持将文件缓存和匿名内存（如进程的堆栈、堆）分开管理，帮助系统管理员区分不同类型的内存使用：

  - memory.kmem.*：这些文件专门用于管理内核内存使用的统计信息和限制。
7.  页面回收与优先级
   内存子系统还能够通过页面回收和优先级机制，确保重要的进程或 cgroup 拥有更高的内存优先级。


### 重要文件
- memory.limit_in_bytes ： 当前内存限制的大小，单位是字节（bytes）。
  ```bash
  536870912
  ```
  这里表示限制为 512 MB。

- memory.soft_limit_in_bytes : 内存的软性限制，单位是字节。软性限制是系统回收内存时的参考，未必严格执行。
  ```bash
  268435456
  ```
  这里表示软性限制为 256 MB。

- memory.memsw.limit_in_bytes ：内存和交换空间（swap）的总限制，单位是字节。
  ```bash
  1073741824
  ```
  这里表示内存和 swap 的总限制为 1 GB。
- memory.usage_in_bytes ：当前 cgroup 使用的物理内存量，单位是字节。
  ```bash
  157286400
  ```
-  memory.max_usage_in_bytes ：自 cgroup 创建以来，cgroup 内进程使用的最大物理内存量，单位是字节。

- memory.failcnt：该 cgroup 内的进程尝试分配超过内存限制时的失败次数（即触发内存限制的次数）
  ```bash
  42
  ```
  这里表示该 cgroup 曾经 42 次尝试分配超过限制的内存。

- memory.stat：内存使用统计。格式是 key value 对。
  ```bash
  cache 12345678
  rss 4567890
  mapped_file 2345678
  swap 123456
  inactive_anon 234567
  active_anon 345678
  inactive_file 123456
  active_file 234567
  unevictable 0
  hierarchical_memory_limit 1073741824
  hierarchical_memsw_limit 2147483648
  ```
  - cache：文件系统缓存的大小。
  - rss：进程实际使用的物理内存，不包括 swap。
  - mapped_file：映射到进程地址空间的文件的大小。
  - swap：使用的 swap 大小。
  - inactive_anon/active_anon：匿名内存页的使用情况（分为活跃和非活跃）。
  - inactive_file/active_file：文件内存页的使用情况（分为活跃和非活跃）。
  - unevictable：无法驱逐的内存页数量。
  - hierarchical_memory_limit：该层级（及子层级）的内存限制。
  - hierarchical_memsw_limit：该层级（及子层级）的内存+swap 限制。

- memory.oom_control ： 用于控制 OOM（Out of Memory）行为。可以设置是否允许内核在该 cgroup 中触发 OOM 杀手。
  ```bash
  oom_kill_disable 0
  under_oom 0
  ```
  - oom_kill_disable 0：表示没有禁用 OOM 杀手，即当内存不足时，内核允许杀掉进程。
  - under_oom 0：表示当前没有进程受到 OOM 的影响。
- memory.swappiness ：这个值表示内存和 swap 之间的使用权重，范围为 0 到 100。0 表示尽可能不使用 swap，100 表示尽量使用 swap。

- memory.kmem.usage_in_bytes ：该文件显示该 cgroup 中使用的内核内存（kernel memory）大小，单位是字节。
  ```bash
  10485760
  ```
  这里表示使用了 10 MB 的内核内存。



## cgroup2


- memory.max：设置或读取该 cgroup 的最大内存限制，单位为字节。max 表示没有限制。

- memory.swap.max：设置或读取该 cgroup 可用的最大 swap 空间，单位为字节。max 表示没有限制。

- memory.current：读取该 cgroup 当前使用的物理内存量，单位为字节。

- memory.events：包含与内存使用相关的事件信息，包括 OOM 事件及内存超出限制的次数。
  ```bash
  low 0
  high 3
  max 2
  oom 1
  ```
  - low：cgroup 达到 memory.low 的次数。
  - high：cgroup 超出 memory.high 的次数。
  - max：cgroup 超出 memory.max 的次数。
  - oom：发生 OOM（Out Of Memory）事件的次数。
- memory.oom.group：控制该 cgroup 的 OOM 行为，如果设置为 1，当内存不足时整个 cgroup 会作为一个整体处理。

- memory.stat：提供详细的内存使用统计信息，包括匿名内存、文件缓存、页面回收等。格式为 key value，键值对表示内存的不同类型和使用量。
  ```bash
  anon 16384000
  file 32768000
  kernel_stack 1024000
  slab 2048000
  sock 512000
  shmem 8192000
  file_mapped 4096000
  file_dirty 512000
  file_writeback 1024000
  pgpgin 4096
  pgpgout 8192
  ```
  - anon：匿名内存使用量。
  - file：文件缓存内存使用量。
  - kernel_stack：内核栈使用量。
  - slab：内核 slab 缓存使用量。
  - sock：socket 缓存使用量。
  - shmem：共享内存使用量。
  - pgpgin 和 pgpgout：页面调入和调出次数。
  
- memory.pressure：显示该 cgroup 的内存压力信息，通常以百分比形式呈现，反映内存不足时的 stall（阻塞）情况。

## 使用示例：以group2为例

1. 在 `/sys/fs/cgroup` 文件夹下创建文件
   ```bash
   sudo mkdir mycgroup
   ```
2. 查看内存限制
   ```bash
   cat memory.max
   ```
3. 写入 128m 限制到文件
  ```bash
  sudo echo 128m |sudo   tee   memory.max 
  128m
  ```
4. 查看内存限制
  ```bash
  cat memory.max 
  134217728
  ```

5. 将当前终端写入该 cgroup 
   ```bash
  sudo echo $$ | sudo tee cgroup.procs 
  3182
   ```
6. 安装 memtester 测试工具，执行内存压测
  ```bash
  sudo apt-get install memtester
  ```
  分配 526m 内存
  ```bash
  memtester 256M 1
  memtester version 4.5.1 (64-bit)
  Copyright (C) 2001-2020 Charles Cazabon.
  Licensed under the GNU General Public License version 2 (only).

  pagesize is 4096
  pagesizemask is 0xfffffffffffff000
  want 256MB (268435456 bytes)
  got  256MB (268435456 bytes), trying mlock ...已杀死
  ```
  可以看到 cgroup 已经将 memtester 杀死，这表明无法分配 256m内存

  尝试分配 120m 内存
  ```bash
  memtester 120M 1
  memtester version 4.5.1 (64-bit)
  Copyright (C) 2001-2020 Charles Cazabon.
  Licensed under the GNU General Public License version 2 (only).

  pagesize is 4096
  pagesizemask is 0xfffffffffffff000
  want 120MB (125829120 bytes)
  got  120MB (125829120 bytes), trying mlock ...locked.
  Loop 1/1:
    Stuck Address       : ok         
    Random Value        : ok
    Compare XOR         : ok
    Compare SUB         : ok
    Compare MUL         : ok
    Compare DIV         : ok
    Compare OR          : ok
    Compare AND         : ok
    Sequential Increment: ok
    Solid Bits          : ok         
    Block Sequential    : ok         
    Checkerboard        : ok         
    Bit Spread          : ok         
    Bit Flip            : ok         
    Walking Ones        : ok         
    Walking Zeroes      : ok         
    8-bit Writes        : ok
    16-bit Writes       : ok

  Done.
  ```
  这里可以看到，此时 memtester 已经成功申请到 500M 内存并且正常完成了内存测试

# 删除 cgroup

删除创建的文件夹即可

```bash
what@what-virtual-machine:/sys/fs/cgroup$ sudo rmdir mycgroup/
[sudo] what 的密码： 
rmdir: 删除 'mycgroup/' 失败: 设备或资源忙
```

# Docker 是如何使用cgroups的？

首先，我们使用以下命令创建一个 nginx 容器：
```bash
docker run -it -m=1g nginx
```
通过 docker inspect 找到容器在宿主机上的pid
```bash
docker inspect 4e08705a812a2d | grep -i pid
    "Pid": 7129,
    "PidMode": "",
    "PidsLimit": null,
```
进入 proc 文件系统找到对应 cgroup 目录
```bash
cd /proc/7129/
```
查看 cgroup 文件即可找到，对应的 cgroup 配置路径
```bash
sudo cat cgroup 
0::/system.slice/docker-4e08705a812a2d204b206511a834a379b88d33b9d3d7344eadfc5a88434ca71f.scope
```
- `0::` ：这部分在cgroup v2中通常表示cgroup的层级或控制器的标识，但在cgroup v2的扁平结构中，这个值可能不总是那么重要或具有直接意义。不过，它确实表明这是一个cgroup v2的路径。
- `/`：这是cgroup v2层次结构的根目录。
- `system.slice/`：这是systemd使用的cgroup切片（slice），用于组织和管理系统服务。system.slice是顶级切片，它包含了所有由systemd直接管理的服务
- `/docker-4e08705a812a2d204b206511a834a379b88d33b9d3d7344eadfc5a88434ca71f.scope` : 这是一个由Docker创建的cgroup作用域（scope），用于隔离和管理单个容器。Docker会为每个容器创建一个唯一的cgroup作用域，该作用域的名称通常包含容器的短ID或完整ID的一部分，以及一个.scope后缀

此时我们只获得了cgroup配置的部分路径，接下来我们查看 cgroup 的挂载点
```bash
mount | grep cgroup  
# 或者  
cat /proc/mounts | grep cgroup
```
输出
```bash
cgroup2 on /sys/fs/cgroup type cgroup2 (rw,nosuid,nodev,noexec,relatime,nsdelegate,memory_recursiveprot)
```
可以看到 cgroup2 挂载点在 `/sys/fs/cgroup` 路径下，我们将两个路径结合在一起

`/sys/fs/cgroup/system.slice/docker-4e08705a812a2d204b206511a834a379b88d33b9d3d7344eadfc5a88434ca71f.scope` 该路径就是我们容器的cgroup配置文件所在的路径

我们进入该路径，随后查看 `memory.max` 文件，单位是字节
```bash
 cat memory.max 
1073741824
```
经过换算得出为 `1G` ,与设置的相符

# proc文件系统


Linux 的 `/proc` 文件系统是一个虚拟文件系统，提供了关于系统和进程的信息。它并不存储在硬盘上，而是内存中动态生成的文件系统，专门用来提供系统和进程的信息。

/proc 文件系统的核心功能是作为内核与用户空间的桥梁，使用户和管理员可以查询内核数据和调试系统状态。/proc 包含许多关于内核、硬件和当前正在运行的进程的信息。

## 系统级文件

系统级文件
- `/proc` 包含一些反映系统全局状态的文件，这些文件提供关于内核、CPU、内存、设备等的信息。
常见的系统级文件和目录包括：

- `/proc/cpuinfo`：包含系统 CPU 的详细信息，例如型号、核心数、频率等。
- `/proc/meminfo`：显示系统内存的使用情况，包括可用的物理内存和交换空间等。
- `/proc/swaps`：列出当前系统中交换分区的使用情况。
- `/proc/uptime`：显示系统从启动到当前的时间以及系统的空闲时间。
- `/proc/version`：显示 Linux 内核的版本信息和编译时间。



## 进程级目录

每个正在运行的进程都有一个对应的目录，位于 `/proc` 文件系统下，名称为进程的 PID（进程 ID），即 `/proc/<pid>`，介绍如下



- `/proc/<pid>/stat`： 这个文件包含了进程的基本状态信息，包括进程的状态、优先级、调度策略、内存使用、CPU 时间等。它包含多个字段，可以使用空格分隔。

- `/proc/<pid>/status`： 提供了更易读的进程状态信息，包括进程的名称、状态、用户和组 ID、内存使用（虚拟内存和物理内存）、信号信息等。

- `/proc/<pid>/cmdline`： 显示启动进程时使用的命令行参数，以 null 字符分隔。

- `/proc/<pid>/environ`： 显示进程的环境变量，变量之间用 null 字符分隔。

- `/proc/<pid>/cwd`： 是一个符号链接，指向进程的当前工作目录。

- `/proc/<pid>/exe`： 也是一个符号链接，指向执行该进程的可执行文件。

- `/proc/<pid>/fd/`： 这个目录包含了该进程打开的文件描述符的符号链接，每个文件描述符指向相应的打开文件。

- `/proc/<pid>/maps`： 显示该进程的内存映射，包括加载的库、内存区域的地址范围及其权限等。

- `/proc/<pid>/task/`： 该目录包含与进程相关的线程信息，每个线程都有一个子目录，名称为其线程的 PID。